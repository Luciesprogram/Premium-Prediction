2023-06-30 20:50:37,228:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 20:50:37,228:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 20:50:37,228:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 20:50:37,228:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 20:50:38,653:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-06-30 20:51:12,487:INFO:PyCaret RegressionExperiment
2023-06-30 20:51:12,487:INFO:Logging name: reg-default-name
2023-06-30 20:51:12,487:INFO:ML Usecase: MLUsecase.REGRESSION
2023-06-30 20:51:12,487:INFO:version 3.0.3
2023-06-30 20:51:12,487:INFO:Initializing setup()
2023-06-30 20:51:12,487:INFO:self.USI: 8a1f
2023-06-30 20:51:12,487:INFO:self._variable_keys: {'fold_shuffle_param', 'X', 'X_train', 'html_param', 'fold_generator', 'log_plots_param', 'pipeline', 'idx', 'n_jobs_param', '_available_plots', 'logging_param', 'transform_target_param', 'gpu_n_jobs_param', 'y', 'exp_name_log', '_ml_usecase', 'y_test', 'y_train', 'target_param', 'data', 'gpu_param', 'exp_id', 'memory', 'fold_groups_param', 'USI', 'X_test', 'seed'}
2023-06-30 20:51:12,487:INFO:Checking environment
2023-06-30 20:51:12,487:INFO:python_version: 3.10.6
2023-06-30 20:51:12,487:INFO:python_build: ('tags/v3.10.6:9c7b4bd', 'Aug  1 2022 21:53:49')
2023-06-30 20:51:12,487:INFO:machine: AMD64
2023-06-30 20:51:12,487:INFO:platform: Windows-10-10.0.22621-SP0
2023-06-30 20:51:12,495:INFO:Memory: svmem(total=6378106880, available=997371904, percent=84.4, used=5380734976, free=997371904)
2023-06-30 20:51:12,495:INFO:Physical Core: 4
2023-06-30 20:51:12,495:INFO:Logical Core: 4
2023-06-30 20:51:12,495:INFO:Checking libraries
2023-06-30 20:51:12,495:INFO:System:
2023-06-30 20:51:12,495:INFO:    python: 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]
2023-06-30 20:51:12,495:INFO:executable: c:\Python310\python.exe
2023-06-30 20:51:12,495:INFO:   machine: Windows-10-10.0.22621-SP0
2023-06-30 20:51:12,495:INFO:PyCaret required dependencies:
2023-06-30 20:51:12,503:INFO:                 pip: 23.1.2
2023-06-30 20:51:12,503:INFO:          setuptools: 63.2.0
2023-06-30 20:51:12,503:INFO:             pycaret: 3.0.3
2023-06-30 20:51:12,503:INFO:             IPython: 8.14.0
2023-06-30 20:51:12,503:INFO:          ipywidgets: 8.0.6
2023-06-30 20:51:12,503:INFO:                tqdm: 4.65.0
2023-06-30 20:51:12,503:INFO:               numpy: 1.23.5
2023-06-30 20:51:12,503:INFO:              pandas: 1.5.3
2023-06-30 20:51:12,503:INFO:              jinja2: 3.1.2
2023-06-30 20:51:12,503:INFO:               scipy: 1.10.1
2023-06-30 20:51:12,503:INFO:              joblib: 1.2.0
2023-06-30 20:51:12,503:INFO:             sklearn: 1.2.2
2023-06-30 20:51:12,503:INFO:                pyod: 1.1.0
2023-06-30 20:51:12,503:INFO:            imblearn: 0.10.1
2023-06-30 20:51:12,503:INFO:   category_encoders: 2.6.1
2023-06-30 20:51:12,503:INFO:            lightgbm: 3.3.5
2023-06-30 20:51:12,503:INFO:               numba: 0.57.1
2023-06-30 20:51:12,503:INFO:            requests: 2.28.2
2023-06-30 20:51:12,503:INFO:          matplotlib: 3.7.1
2023-06-30 20:51:12,503:INFO:          scikitplot: 0.3.7
2023-06-30 20:51:12,503:INFO:         yellowbrick: 1.5
2023-06-30 20:51:12,503:INFO:              plotly: 5.15.0
2023-06-30 20:51:12,503:INFO:    plotly-resampler: Not installed
2023-06-30 20:51:12,503:INFO:             kaleido: 0.2.1
2023-06-30 20:51:12,503:INFO:           schemdraw: 0.15
2023-06-30 20:51:12,503:INFO:         statsmodels: 0.14.0
2023-06-30 20:51:12,503:INFO:              sktime: 0.20.0
2023-06-30 20:51:12,503:INFO:               tbats: 1.1.3
2023-06-30 20:51:12,503:INFO:            pmdarima: 2.0.3
2023-06-30 20:51:12,503:INFO:              psutil: 5.9.5
2023-06-30 20:51:12,503:INFO:          markupsafe: 2.1.2
2023-06-30 20:51:12,503:INFO:             pickle5: Not installed
2023-06-30 20:51:12,503:INFO:         cloudpickle: 2.2.1
2023-06-30 20:51:12,503:INFO:         deprecation: 2.1.0
2023-06-30 20:51:12,503:INFO:              xxhash: 3.2.0
2023-06-30 20:51:12,503:INFO:           wurlitzer: Not installed
2023-06-30 20:51:12,503:INFO:PyCaret optional dependencies:
2023-06-30 20:51:12,535:INFO:                shap: Not installed
2023-06-30 20:51:12,535:INFO:           interpret: Not installed
2023-06-30 20:51:12,543:INFO:                umap: Not installed
2023-06-30 20:51:12,543:INFO:    pandas_profiling: Not installed
2023-06-30 20:51:12,543:INFO:  explainerdashboard: Not installed
2023-06-30 20:51:12,543:INFO:             autoviz: Not installed
2023-06-30 20:51:12,543:INFO:           fairlearn: Not installed
2023-06-30 20:51:12,543:INFO:          deepchecks: Not installed
2023-06-30 20:51:12,543:INFO:             xgboost: Not installed
2023-06-30 20:51:12,543:INFO:            catboost: Not installed
2023-06-30 20:51:12,543:INFO:              kmodes: Not installed
2023-06-30 20:51:12,543:INFO:             mlxtend: Not installed
2023-06-30 20:51:12,543:INFO:       statsforecast: Not installed
2023-06-30 20:51:12,543:INFO:        tune_sklearn: Not installed
2023-06-30 20:51:12,543:INFO:                 ray: Not installed
2023-06-30 20:51:12,543:INFO:            hyperopt: Not installed
2023-06-30 20:51:12,543:INFO:              optuna: Not installed
2023-06-30 20:51:12,543:INFO:               skopt: Not installed
2023-06-30 20:51:12,543:INFO:              mlflow: Not installed
2023-06-30 20:51:12,543:INFO:              gradio: Not installed
2023-06-30 20:51:12,543:INFO:             fastapi: Not installed
2023-06-30 20:51:12,543:INFO:             uvicorn: Not installed
2023-06-30 20:51:12,543:INFO:              m2cgen: Not installed
2023-06-30 20:51:12,543:INFO:           evidently: Not installed
2023-06-30 20:51:12,543:INFO:               fugue: Not installed
2023-06-30 20:51:12,543:INFO:           streamlit: 1.20.0
2023-06-30 20:51:12,543:INFO:             prophet: Not installed
2023-06-30 20:51:12,543:INFO:None
2023-06-30 20:51:12,543:INFO:Set up data.
2023-06-30 20:51:12,559:INFO:Set up train/test split.
2023-06-30 20:51:12,567:INFO:Set up index.
2023-06-30 20:51:12,567:INFO:Set up folding strategy.
2023-06-30 20:51:12,567:INFO:Assigning column types.
2023-06-30 20:51:12,591:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-06-30 20:51:12,591:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-06-30 20:51:12,599:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-30 20:51:12,615:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 20:51:12,768:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 20:51:12,857:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 20:51:12,857:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:12,897:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:12,897:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-06-30 20:51:12,905:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-30 20:51:12,921:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 20:51:13,045:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 20:51:13,139:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 20:51:13,139:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:13,139:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:13,139:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-06-30 20:51:13,147:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-30 20:51:13,163:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 20:51:13,355:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 20:51:13,452:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 20:51:13,452:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:13,452:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:13,468:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-30 20:51:13,479:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 20:51:13,614:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 20:51:13,711:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 20:51:13,711:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:13,711:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:13,711:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-06-30 20:51:13,735:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 20:51:13,863:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 20:51:14,063:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 20:51:14,063:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:14,063:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:14,087:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 20:51:14,248:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 20:51:14,363:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 20:51:14,363:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:14,363:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:14,363:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-06-30 20:51:14,521:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 20:51:14,618:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 20:51:14,627:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:14,627:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:14,796:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 20:51:14,896:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 20:51:14,896:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:14,896:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:14,896:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-06-30 20:51:15,079:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 20:51:15,185:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:15,185:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:15,338:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 20:51:15,443:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:15,451:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:15,451:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-06-30 20:51:15,702:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:15,702:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:15,951:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:15,951:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:15,951:INFO:Preparing preprocessing pipeline...
2023-06-30 20:51:15,951:INFO:Set up simple imputation.
2023-06-30 20:51:15,963:INFO:Set up encoding of ordinal features.
2023-06-30 20:51:15,968:INFO:Set up encoding of categorical features.
2023-06-30 20:51:15,968:INFO:Set up feature normalization.
2023-06-30 20:51:16,181:INFO:Finished creating preprocessing pipeline.
2023-06-30 20:51:16,251:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\acer\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWr...
                                                                         'data_type': dtype('O'),
                                                                         'mapping': female    0
male      1
NaN      -1
dtype: int64},
                                                                        {'col': 'smoker',
                                                                         'data_type': dtype('O'),
                                                                         'mapping': no     0
yes    1
NaN   -1
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2023-06-30 20:51:16,251:INFO:Creating final display dataframe.
2023-06-30 20:51:16,775:INFO:Setup _display_container:                     Description             Value
0                    Session id              9085
1                        Target          expenses
2                   Target type        Regression
3           Original data shape         (1338, 7)
4        Transformed data shape        (1338, 10)
5   Transformed train set shape        (1070, 10)
6    Transformed test set shape         (268, 10)
7              Ordinal features                 2
8              Numeric features                 3
9          Categorical features                 3
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16                    Normalize              True
17             Normalize method            zscore
18               Fold Generator             KFold
19                  Fold Number                10
20                     CPU Jobs                -1
21                      Use GPU             False
22               Log Experiment             False
23              Experiment Name  reg-default-name
24                          USI              8a1f
2023-06-30 20:51:17,039:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:17,039:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:17,279:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:17,279:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:17,279:INFO:setup() successfully completed in 4.8s...............
2023-06-30 20:51:17,562:INFO:PyCaret RegressionExperiment
2023-06-30 20:51:17,562:INFO:Logging name: reg-default-name
2023-06-30 20:51:17,562:INFO:ML Usecase: MLUsecase.REGRESSION
2023-06-30 20:51:17,562:INFO:version 3.0.3
2023-06-30 20:51:17,562:INFO:Initializing setup()
2023-06-30 20:51:17,562:INFO:self.USI: 4f05
2023-06-30 20:51:17,562:INFO:self._variable_keys: {'fold_shuffle_param', 'X', 'X_train', 'html_param', 'fold_generator', 'log_plots_param', 'pipeline', 'idx', 'n_jobs_param', '_available_plots', 'logging_param', 'transform_target_param', 'gpu_n_jobs_param', 'y', 'exp_name_log', '_ml_usecase', 'y_test', 'y_train', 'target_param', 'data', 'gpu_param', 'exp_id', 'memory', 'fold_groups_param', 'USI', 'X_test', 'seed'}
2023-06-30 20:51:17,562:INFO:Checking environment
2023-06-30 20:51:17,562:INFO:python_version: 3.10.6
2023-06-30 20:51:17,562:INFO:python_build: ('tags/v3.10.6:9c7b4bd', 'Aug  1 2022 21:53:49')
2023-06-30 20:51:17,562:INFO:machine: AMD64
2023-06-30 20:51:17,562:INFO:platform: Windows-10-10.0.22621-SP0
2023-06-30 20:51:17,569:INFO:Memory: svmem(total=6378106880, available=961290240, percent=84.9, used=5416816640, free=961290240)
2023-06-30 20:51:17,569:INFO:Physical Core: 4
2023-06-30 20:51:17,569:INFO:Logical Core: 4
2023-06-30 20:51:17,569:INFO:Checking libraries
2023-06-30 20:51:17,569:INFO:System:
2023-06-30 20:51:17,569:INFO:    python: 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]
2023-06-30 20:51:17,569:INFO:executable: c:\Python310\python.exe
2023-06-30 20:51:17,569:INFO:   machine: Windows-10-10.0.22621-SP0
2023-06-30 20:51:17,569:INFO:PyCaret required dependencies:
2023-06-30 20:51:17,569:INFO:                 pip: 23.1.2
2023-06-30 20:51:17,569:INFO:          setuptools: 63.2.0
2023-06-30 20:51:17,569:INFO:             pycaret: 3.0.3
2023-06-30 20:51:17,569:INFO:             IPython: 8.14.0
2023-06-30 20:51:17,569:INFO:          ipywidgets: 8.0.6
2023-06-30 20:51:17,569:INFO:                tqdm: 4.65.0
2023-06-30 20:51:17,569:INFO:               numpy: 1.23.5
2023-06-30 20:51:17,569:INFO:              pandas: 1.5.3
2023-06-30 20:51:17,569:INFO:              jinja2: 3.1.2
2023-06-30 20:51:17,569:INFO:               scipy: 1.10.1
2023-06-30 20:51:17,569:INFO:              joblib: 1.2.0
2023-06-30 20:51:17,569:INFO:             sklearn: 1.2.2
2023-06-30 20:51:17,569:INFO:                pyod: 1.1.0
2023-06-30 20:51:17,569:INFO:            imblearn: 0.10.1
2023-06-30 20:51:17,569:INFO:   category_encoders: 2.6.1
2023-06-30 20:51:17,569:INFO:            lightgbm: 3.3.5
2023-06-30 20:51:17,569:INFO:               numba: 0.57.1
2023-06-30 20:51:17,569:INFO:            requests: 2.28.2
2023-06-30 20:51:17,569:INFO:          matplotlib: 3.7.1
2023-06-30 20:51:17,569:INFO:          scikitplot: 0.3.7
2023-06-30 20:51:17,569:INFO:         yellowbrick: 1.5
2023-06-30 20:51:17,569:INFO:              plotly: 5.15.0
2023-06-30 20:51:17,569:INFO:    plotly-resampler: Not installed
2023-06-30 20:51:17,569:INFO:             kaleido: 0.2.1
2023-06-30 20:51:17,569:INFO:           schemdraw: 0.15
2023-06-30 20:51:17,569:INFO:         statsmodels: 0.14.0
2023-06-30 20:51:17,569:INFO:              sktime: 0.20.0
2023-06-30 20:51:17,569:INFO:               tbats: 1.1.3
2023-06-30 20:51:17,569:INFO:            pmdarima: 2.0.3
2023-06-30 20:51:17,569:INFO:              psutil: 5.9.5
2023-06-30 20:51:17,569:INFO:          markupsafe: 2.1.2
2023-06-30 20:51:17,569:INFO:             pickle5: Not installed
2023-06-30 20:51:17,569:INFO:         cloudpickle: 2.2.1
2023-06-30 20:51:17,569:INFO:         deprecation: 2.1.0
2023-06-30 20:51:17,569:INFO:              xxhash: 3.2.0
2023-06-30 20:51:17,577:INFO:           wurlitzer: Not installed
2023-06-30 20:51:17,577:INFO:PyCaret optional dependencies:
2023-06-30 20:51:17,577:INFO:                shap: Not installed
2023-06-30 20:51:17,577:INFO:           interpret: Not installed
2023-06-30 20:51:17,577:INFO:                umap: Not installed
2023-06-30 20:51:17,577:INFO:    pandas_profiling: Not installed
2023-06-30 20:51:17,577:INFO:  explainerdashboard: Not installed
2023-06-30 20:51:17,577:INFO:             autoviz: Not installed
2023-06-30 20:51:17,577:INFO:           fairlearn: Not installed
2023-06-30 20:51:17,577:INFO:          deepchecks: Not installed
2023-06-30 20:51:17,577:INFO:             xgboost: Not installed
2023-06-30 20:51:17,577:INFO:            catboost: Not installed
2023-06-30 20:51:17,577:INFO:              kmodes: Not installed
2023-06-30 20:51:17,577:INFO:             mlxtend: Not installed
2023-06-30 20:51:17,577:INFO:       statsforecast: Not installed
2023-06-30 20:51:17,577:INFO:        tune_sklearn: Not installed
2023-06-30 20:51:17,579:INFO:                 ray: Not installed
2023-06-30 20:51:17,579:INFO:            hyperopt: Not installed
2023-06-30 20:51:17,579:INFO:              optuna: Not installed
2023-06-30 20:51:17,579:INFO:               skopt: Not installed
2023-06-30 20:51:17,579:INFO:              mlflow: Not installed
2023-06-30 20:51:17,579:INFO:              gradio: Not installed
2023-06-30 20:51:17,579:INFO:             fastapi: Not installed
2023-06-30 20:51:17,579:INFO:             uvicorn: Not installed
2023-06-30 20:51:17,579:INFO:              m2cgen: Not installed
2023-06-30 20:51:17,579:INFO:           evidently: Not installed
2023-06-30 20:51:17,579:INFO:               fugue: Not installed
2023-06-30 20:51:17,579:INFO:           streamlit: 1.20.0
2023-06-30 20:51:17,579:INFO:             prophet: Not installed
2023-06-30 20:51:17,579:INFO:None
2023-06-30 20:51:17,579:INFO:Set up data.
2023-06-30 20:51:17,585:INFO:Set up train/test split.
2023-06-30 20:51:17,596:INFO:Set up index.
2023-06-30 20:51:17,596:INFO:Set up folding strategy.
2023-06-30 20:51:17,596:INFO:Assigning column types.
2023-06-30 20:51:17,601:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-06-30 20:51:17,601:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-06-30 20:51:17,617:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-30 20:51:17,625:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 20:51:17,753:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 20:51:17,882:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 20:51:17,882:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:17,882:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:17,882:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-06-30 20:51:17,906:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-30 20:51:17,922:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 20:51:18,074:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 20:51:18,194:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 20:51:18,202:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:18,202:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:18,202:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-06-30 20:51:18,218:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-30 20:51:18,226:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 20:51:18,386:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 20:51:18,522:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 20:51:18,522:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:18,522:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:18,554:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-30 20:51:18,602:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 20:51:18,867:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 20:51:18,979:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 20:51:18,979:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:18,979:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:18,979:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-06-30 20:51:19,003:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 20:51:19,141:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 20:51:19,245:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 20:51:19,245:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:19,245:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:19,264:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 20:51:19,391:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 20:51:19,487:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 20:51:19,495:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:19,495:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:19,495:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-06-30 20:51:19,687:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 20:51:19,794:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 20:51:19,794:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:19,794:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:19,945:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 20:51:20,049:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 20:51:20,049:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:20,049:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:20,049:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-06-30 20:51:20,234:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 20:51:20,354:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:20,354:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:20,530:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 20:51:20,635:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:20,635:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:20,635:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-06-30 20:51:20,884:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:20,884:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:21,128:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:21,128:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:21,128:INFO:Preparing preprocessing pipeline...
2023-06-30 20:51:21,134:INFO:Set up simple imputation.
2023-06-30 20:51:21,134:INFO:Set up encoding of ordinal features.
2023-06-30 20:51:21,142:INFO:Set up encoding of categorical features.
2023-06-30 20:51:21,142:INFO:Set up polynomial features.
2023-06-30 20:51:21,142:INFO:Set up binning of numerical features.
2023-06-30 20:51:21,142:INFO:Set up feature normalization.
2023-06-30 20:51:21,797:INFO:Finished creating preprocessing pipeline.
2023-06-30 20:51:21,869:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\acer\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWr...
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('bin_numeric_features',
                 TransformerWrapper(include=['age', 'bmi'],
                                    transformer=KBinsDiscretizer(encode='ordinal',
                                                                 strategy='kmeans'))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2023-06-30 20:51:21,869:INFO:Creating final display dataframe.
2023-06-30 20:51:22,499:INFO:Setup _display_container:                     Description             Value
0                    Session id              6820
1                        Target          expenses
2                   Target type        Regression
3           Original data shape         (1338, 7)
4        Transformed data shape        (1338, 55)
5   Transformed train set shape         (936, 55)
6    Transformed test set shape         (402, 55)
7              Ordinal features                 2
8              Numeric features                 3
9          Categorical features                 3
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16          Polynomial features              True
17            Polynomial degree                 2
18                    Normalize              True
19             Normalize method            zscore
20               Fold Generator             KFold
21                  Fold Number                10
22                     CPU Jobs                -1
23                      Use GPU             False
24               Log Experiment             False
25              Experiment Name  reg-default-name
26                          USI              4f05
2023-06-30 20:51:22,773:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:22,773:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:23,015:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:23,015:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 20:51:23,015:INFO:setup() successfully completed in 5.46s...............
2023-06-30 20:51:23,079:INFO:Initializing compare_models()
2023-06-30 20:51:23,079:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, include=None, fold=None, round=4, cross_validation=True, sort=RMSE, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'RMSE', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-06-30 20:51:23,079:INFO:Checking exceptions
2023-06-30 20:51:23,079:INFO:Preparing display monitor
2023-06-30 20:51:23,151:INFO:Initializing Linear Regression
2023-06-30 20:51:23,151:INFO:Total runtime is 0.0 minutes
2023-06-30 20:51:23,159:INFO:SubProcess create_model() called ==================================
2023-06-30 20:51:23,159:INFO:Initializing create_model()
2023-06-30 20:51:23,159:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7D52F1C0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 20:51:23,159:INFO:Checking exceptions
2023-06-30 20:51:23,159:INFO:Importing libraries
2023-06-30 20:51:23,159:INFO:Copying training dataset
2023-06-30 20:51:23,175:INFO:Defining folds
2023-06-30 20:51:23,175:INFO:Declaring metric variables
2023-06-30 20:51:23,183:INFO:Importing untrained model
2023-06-30 20:51:23,191:INFO:Linear Regression Imported successfully
2023-06-30 20:51:23,207:INFO:Starting cross validation
2023-06-30 20:51:23,295:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 20:51:35,439:INFO:Calculating mean and std
2023-06-30 20:51:35,439:INFO:Creating metrics dataframe
2023-06-30 20:51:35,471:INFO:Uploading results into container
2023-06-30 20:51:35,479:INFO:Uploading model into container now
2023-06-30 20:51:35,479:INFO:_master_model_container: 1
2023-06-30 20:51:35,479:INFO:_display_container: 2
2023-06-30 20:51:35,479:INFO:LinearRegression(n_jobs=-1)
2023-06-30 20:51:35,479:INFO:create_model() successfully completed......................................
2023-06-30 20:51:35,703:INFO:SubProcess create_model() end ==================================
2023-06-30 20:51:35,703:INFO:Creating metrics dataframe
2023-06-30 20:51:35,727:INFO:Initializing Lasso Regression
2023-06-30 20:51:35,727:INFO:Total runtime is 0.20960758527119955 minutes
2023-06-30 20:51:35,735:INFO:SubProcess create_model() called ==================================
2023-06-30 20:51:35,735:INFO:Initializing create_model()
2023-06-30 20:51:35,735:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7D52F1C0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 20:51:35,735:INFO:Checking exceptions
2023-06-30 20:51:35,735:INFO:Importing libraries
2023-06-30 20:51:35,735:INFO:Copying training dataset
2023-06-30 20:51:35,743:INFO:Defining folds
2023-06-30 20:51:35,751:INFO:Declaring metric variables
2023-06-30 20:51:35,759:INFO:Importing untrained model
2023-06-30 20:51:35,767:INFO:Lasso Regression Imported successfully
2023-06-30 20:51:35,783:INFO:Starting cross validation
2023-06-30 20:51:35,791:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 20:51:36,384:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.890e+09, tolerance: 1.264e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-30 20:51:36,392:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.313e+09, tolerance: 1.291e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-30 20:51:36,400:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.768e+09, tolerance: 1.254e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-30 20:51:37,241:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.175e+09, tolerance: 1.271e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-30 20:51:37,265:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.334e+09, tolerance: 1.193e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-30 20:51:37,281:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.819e+09, tolerance: 1.298e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-30 20:51:37,409:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.084e+09, tolerance: 1.313e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-30 20:51:38,017:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.581e+09, tolerance: 1.283e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-30 20:51:38,025:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.935e+09, tolerance: 1.259e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-30 20:51:38,345:INFO:Calculating mean and std
2023-06-30 20:51:38,353:INFO:Creating metrics dataframe
2023-06-30 20:51:38,393:INFO:Uploading results into container
2023-06-30 20:51:38,393:INFO:Uploading model into container now
2023-06-30 20:51:38,393:INFO:_master_model_container: 2
2023-06-30 20:51:38,393:INFO:_display_container: 2
2023-06-30 20:51:38,393:INFO:Lasso(random_state=6820)
2023-06-30 20:51:38,393:INFO:create_model() successfully completed......................................
2023-06-30 20:51:38,649:INFO:SubProcess create_model() end ==================================
2023-06-30 20:51:38,649:INFO:Creating metrics dataframe
2023-06-30 20:51:38,665:INFO:Initializing Ridge Regression
2023-06-30 20:51:38,665:INFO:Total runtime is 0.25858118136723834 minutes
2023-06-30 20:51:38,681:INFO:SubProcess create_model() called ==================================
2023-06-30 20:51:38,681:INFO:Initializing create_model()
2023-06-30 20:51:38,681:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7D52F1C0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 20:51:38,681:INFO:Checking exceptions
2023-06-30 20:51:38,681:INFO:Importing libraries
2023-06-30 20:51:38,681:INFO:Copying training dataset
2023-06-30 20:51:38,689:INFO:Defining folds
2023-06-30 20:51:38,689:INFO:Declaring metric variables
2023-06-30 20:51:38,697:INFO:Importing untrained model
2023-06-30 20:51:38,705:INFO:Ridge Regression Imported successfully
2023-06-30 20:51:38,729:INFO:Starting cross validation
2023-06-30 20:51:38,729:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 20:51:41,267:INFO:Calculating mean and std
2023-06-30 20:51:41,267:INFO:Creating metrics dataframe
2023-06-30 20:51:41,315:INFO:Uploading results into container
2023-06-30 20:51:41,315:INFO:Uploading model into container now
2023-06-30 20:51:41,315:INFO:_master_model_container: 3
2023-06-30 20:51:41,315:INFO:_display_container: 2
2023-06-30 20:51:41,315:INFO:Ridge(random_state=6820)
2023-06-30 20:51:41,315:INFO:create_model() successfully completed......................................
2023-06-30 20:51:41,499:INFO:SubProcess create_model() end ==================================
2023-06-30 20:51:41,499:INFO:Creating metrics dataframe
2023-06-30 20:51:41,515:INFO:Initializing Elastic Net
2023-06-30 20:51:41,515:INFO:Total runtime is 0.3060733874638875 minutes
2023-06-30 20:51:41,531:INFO:SubProcess create_model() called ==================================
2023-06-30 20:51:41,531:INFO:Initializing create_model()
2023-06-30 20:51:41,531:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7D52F1C0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 20:51:41,531:INFO:Checking exceptions
2023-06-30 20:51:41,531:INFO:Importing libraries
2023-06-30 20:51:41,531:INFO:Copying training dataset
2023-06-30 20:51:41,555:INFO:Defining folds
2023-06-30 20:51:41,555:INFO:Declaring metric variables
2023-06-30 20:51:41,563:INFO:Importing untrained model
2023-06-30 20:51:41,571:INFO:Elastic Net Imported successfully
2023-06-30 20:51:41,587:INFO:Starting cross validation
2023-06-30 20:51:41,587:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 20:51:43,901:INFO:Calculating mean and std
2023-06-30 20:51:43,909:INFO:Creating metrics dataframe
2023-06-30 20:51:43,957:INFO:Uploading results into container
2023-06-30 20:51:43,965:INFO:Uploading model into container now
2023-06-30 20:51:43,965:INFO:_master_model_container: 4
2023-06-30 20:51:43,965:INFO:_display_container: 2
2023-06-30 20:51:43,965:INFO:ElasticNet(random_state=6820)
2023-06-30 20:51:43,965:INFO:create_model() successfully completed......................................
2023-06-30 20:51:44,149:INFO:SubProcess create_model() end ==================================
2023-06-30 20:51:44,149:INFO:Creating metrics dataframe
2023-06-30 20:51:44,173:INFO:Initializing Least Angle Regression
2023-06-30 20:51:44,173:INFO:Total runtime is 0.35037589867909746 minutes
2023-06-30 20:51:44,181:INFO:SubProcess create_model() called ==================================
2023-06-30 20:51:44,181:INFO:Initializing create_model()
2023-06-30 20:51:44,181:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7D52F1C0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 20:51:44,181:INFO:Checking exceptions
2023-06-30 20:51:44,181:INFO:Importing libraries
2023-06-30 20:51:44,181:INFO:Copying training dataset
2023-06-30 20:51:44,189:INFO:Defining folds
2023-06-30 20:51:44,197:INFO:Declaring metric variables
2023-06-30 20:51:44,205:INFO:Importing untrained model
2023-06-30 20:51:44,213:INFO:Least Angle Regression Imported successfully
2023-06-30 20:51:44,229:INFO:Starting cross validation
2023-06-30 20:51:44,237:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 20:51:44,732:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.520e+02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,732:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=9.484e+01, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,732:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=4.926e+01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,732:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=2.741e+03, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,732:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=9.199e+01, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,732:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=4.267e+01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,734:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=1.889e+03, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,734:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=1.403e+03, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,734:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=8.132e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,734:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=3.163e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,734:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=5.294e+01, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,734:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=9.019e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,734:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=4.965e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,734:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.393e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,734:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=1.914e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,734:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.023e+02, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,734:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=8.610e+01, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,734:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=6.788e+01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,734:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=6.801e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,734:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=4.352e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,734:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=2.731e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,734:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=7.578e+00, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,734:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=6.867e+01, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,743:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=6.591e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,743:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=6.084e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,743:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=4.371e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,743:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=2.418e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,743:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=1.038e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:44,743:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=2.193e+00, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,749:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.435e+02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,749:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.502e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,749:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=6.443e+01, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,749:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=4.750e+01, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,749:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.133e+01, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,749:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.201e+01, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,757:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.240e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,757:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.174e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,757:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=4.797e+00, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,757:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=3.236e+00, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,757:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.140e+00, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,765:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.102e+00, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,765:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.235e+00, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,765:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=6.907e-01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,805:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=1.358e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,805:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=1.325e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,805:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=9.719e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,805:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=9.667e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,805:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=5.427e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,805:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=3.120e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,861:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=5.307e+02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,869:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=5.278e+02, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,869:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=4.341e+03, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,869:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=1.033e+03, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:45,869:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=7.522e+02, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:46,085:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=6.247e+01, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:46,085:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=6.934e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:46,093:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=2.590e+03, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:46,093:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=1.538e+03, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:46,093:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=9.552e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:46,093:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=3.848e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:46,093:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.924e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:46,093:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.603e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:46,093:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=6.116e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:46,565:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.109e+01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:46,565:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=4.002e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:46,573:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=5.970e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:46,573:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=5.465e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:46,573:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=5.251e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:46,573:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=3.215e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:46,573:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=2.218e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:46,573:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=1.308e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:46,630:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.714e+01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:46,630:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=4.910e+01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:46,630:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=4.847e+03, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:46,630:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=3.201e+03, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:46,630:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=1.866e+03, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:46,630:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=1.180e+03, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:46,918:INFO:Calculating mean and std
2023-06-30 20:51:46,918:INFO:Creating metrics dataframe
2023-06-30 20:51:46,974:INFO:Uploading results into container
2023-06-30 20:51:46,982:INFO:Uploading model into container now
2023-06-30 20:51:46,982:INFO:_master_model_container: 5
2023-06-30 20:51:46,982:INFO:_display_container: 2
2023-06-30 20:51:46,982:INFO:Lars(random_state=6820)
2023-06-30 20:51:46,982:INFO:create_model() successfully completed......................................
2023-06-30 20:51:47,166:INFO:SubProcess create_model() end ==================================
2023-06-30 20:51:47,166:INFO:Creating metrics dataframe
2023-06-30 20:51:47,182:INFO:Initializing Lasso Least Angle Regression
2023-06-30 20:51:47,182:INFO:Total runtime is 0.4005245129267374 minutes
2023-06-30 20:51:47,198:INFO:SubProcess create_model() called ==================================
2023-06-30 20:51:47,198:INFO:Initializing create_model()
2023-06-30 20:51:47,198:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7D52F1C0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 20:51:47,198:INFO:Checking exceptions
2023-06-30 20:51:47,198:INFO:Importing libraries
2023-06-30 20:51:47,198:INFO:Copying training dataset
2023-06-30 20:51:47,206:INFO:Defining folds
2023-06-30 20:51:47,214:INFO:Declaring metric variables
2023-06-30 20:51:47,222:INFO:Importing untrained model
2023-06-30 20:51:47,230:INFO:Lasso Least Angle Regression Imported successfully
2023-06-30 20:51:47,246:INFO:Starting cross validation
2023-06-30 20:51:47,254:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 20:51:47,718:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=2.725e+02, previous alpha=2.605e+02, with an active set of 11 regressors.
  warnings.warn(

2023-06-30 20:51:47,790:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.426e+02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:47,790:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=1.705e+02, previous alpha=1.654e+02, with an active set of 15 regressors.
  warnings.warn(

2023-06-30 20:51:47,790:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.262e+02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:47,790:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=5.063e+01, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:47,790:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 27 iterations, alpha=6.094e+01, previous alpha=5.063e+01, with an active set of 22 regressors.
  warnings.warn(

2023-06-30 20:51:47,798:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.678e+01, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:47,806:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=3.647e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:47,806:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=3.535e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:47,806:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 56 iterations, alpha=3.144e+00, previous alpha=3.142e+00, with an active set of 37 regressors.
  warnings.warn(

2023-06-30 20:51:48,510:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 14 iterations, alpha=1.771e+02, previous alpha=1.751e+02, with an active set of 15 regressors.
  warnings.warn(

2023-06-30 20:51:48,822:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.913e+01, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:48,822:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.881e+01, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:48,822:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.577e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:48,822:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 38 iterations, alpha=1.342e+01, previous alpha=1.104e+01, with an active set of 31 regressors.
  warnings.warn(

2023-06-30 20:51:48,886:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 11 iterations, alpha=2.954e+02, previous alpha=2.821e+02, with an active set of 12 regressors.
  warnings.warn(

2023-06-30 20:51:48,983:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=3.328e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:48,983:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=3.278e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:48,983:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=2.553e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:48,991:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=1.616e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:48,991:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 62 iterations, alpha=2.553e+00, previous alpha=1.453e+00, with an active set of 39 regressors.
  warnings.warn(

2023-06-30 20:51:49,383:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.645e+01, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:49,391:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=4.892e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:49,391:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=2.446e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 20:51:49,399:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 59 iterations, alpha=2.372e+00, previous alpha=2.372e+00, with an active set of 36 regressors.
  warnings.warn(

2023-06-30 20:51:49,639:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=4.497e+01, previous alpha=4.387e+01, with an active set of 23 regressors.
  warnings.warn(

2023-06-30 20:51:49,903:INFO:Calculating mean and std
2023-06-30 20:51:49,911:INFO:Creating metrics dataframe
2023-06-30 20:51:49,983:INFO:Uploading results into container
2023-06-30 20:51:49,983:INFO:Uploading model into container now
2023-06-30 20:51:49,983:INFO:_master_model_container: 6
2023-06-30 20:51:49,983:INFO:_display_container: 2
2023-06-30 20:51:49,983:INFO:LassoLars(random_state=6820)
2023-06-30 20:51:49,983:INFO:create_model() successfully completed......................................
2023-06-30 20:51:50,167:INFO:SubProcess create_model() end ==================================
2023-06-30 20:51:50,167:INFO:Creating metrics dataframe
2023-06-30 20:51:50,183:INFO:Initializing Orthogonal Matching Pursuit
2023-06-30 20:51:50,191:INFO:Total runtime is 0.4505466222763061 minutes
2023-06-30 20:51:50,199:INFO:SubProcess create_model() called ==================================
2023-06-30 20:51:50,199:INFO:Initializing create_model()
2023-06-30 20:51:50,199:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7D52F1C0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 20:51:50,199:INFO:Checking exceptions
2023-06-30 20:51:50,199:INFO:Importing libraries
2023-06-30 20:51:50,199:INFO:Copying training dataset
2023-06-30 20:51:50,207:INFO:Defining folds
2023-06-30 20:51:50,207:INFO:Declaring metric variables
2023-06-30 20:51:50,223:INFO:Importing untrained model
2023-06-30 20:51:50,231:INFO:Orthogonal Matching Pursuit Imported successfully
2023-06-30 20:51:50,247:INFO:Starting cross validation
2023-06-30 20:51:50,255:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 20:51:52,672:INFO:Calculating mean and std
2023-06-30 20:51:52,680:INFO:Creating metrics dataframe
2023-06-30 20:51:52,760:INFO:Uploading results into container
2023-06-30 20:51:52,760:INFO:Uploading model into container now
2023-06-30 20:51:52,760:INFO:_master_model_container: 7
2023-06-30 20:51:52,760:INFO:_display_container: 2
2023-06-30 20:51:52,760:INFO:OrthogonalMatchingPursuit()
2023-06-30 20:51:52,760:INFO:create_model() successfully completed......................................
2023-06-30 20:51:52,945:INFO:SubProcess create_model() end ==================================
2023-06-30 20:51:52,945:INFO:Creating metrics dataframe
2023-06-30 20:51:52,969:INFO:Initializing Bayesian Ridge
2023-06-30 20:51:52,969:INFO:Total runtime is 0.4969705939292907 minutes
2023-06-30 20:51:52,977:INFO:SubProcess create_model() called ==================================
2023-06-30 20:51:52,977:INFO:Initializing create_model()
2023-06-30 20:51:52,977:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7D52F1C0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 20:51:52,977:INFO:Checking exceptions
2023-06-30 20:51:52,977:INFO:Importing libraries
2023-06-30 20:51:52,977:INFO:Copying training dataset
2023-06-30 20:51:52,985:INFO:Defining folds
2023-06-30 20:51:52,985:INFO:Declaring metric variables
2023-06-30 20:51:52,993:INFO:Importing untrained model
2023-06-30 20:51:53,009:INFO:Bayesian Ridge Imported successfully
2023-06-30 20:51:53,025:INFO:Starting cross validation
2023-06-30 20:51:53,025:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 20:51:55,649:INFO:Calculating mean and std
2023-06-30 20:51:55,657:INFO:Creating metrics dataframe
2023-06-30 20:51:55,737:INFO:Uploading results into container
2023-06-30 20:51:55,737:INFO:Uploading model into container now
2023-06-30 20:51:55,745:INFO:_master_model_container: 8
2023-06-30 20:51:55,745:INFO:_display_container: 2
2023-06-30 20:51:55,745:INFO:BayesianRidge()
2023-06-30 20:51:55,745:INFO:create_model() successfully completed......................................
2023-06-30 20:51:55,921:INFO:SubProcess create_model() end ==================================
2023-06-30 20:51:55,929:INFO:Creating metrics dataframe
2023-06-30 20:51:55,945:INFO:Initializing Passive Aggressive Regressor
2023-06-30 20:51:55,945:INFO:Total runtime is 0.5465741952260335 minutes
2023-06-30 20:51:55,953:INFO:SubProcess create_model() called ==================================
2023-06-30 20:51:55,961:INFO:Initializing create_model()
2023-06-30 20:51:55,961:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7D52F1C0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 20:51:55,961:INFO:Checking exceptions
2023-06-30 20:51:55,961:INFO:Importing libraries
2023-06-30 20:51:55,961:INFO:Copying training dataset
2023-06-30 20:51:55,977:INFO:Defining folds
2023-06-30 20:51:55,977:INFO:Declaring metric variables
2023-06-30 20:51:55,985:INFO:Importing untrained model
2023-06-30 20:51:55,993:INFO:Passive Aggressive Regressor Imported successfully
2023-06-30 20:51:56,009:INFO:Starting cross validation
2023-06-30 20:51:56,017:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 20:51:59,428:INFO:Calculating mean and std
2023-06-30 20:51:59,436:INFO:Creating metrics dataframe
2023-06-30 20:51:59,540:INFO:Uploading results into container
2023-06-30 20:51:59,540:INFO:Uploading model into container now
2023-06-30 20:51:59,548:INFO:_master_model_container: 9
2023-06-30 20:51:59,548:INFO:_display_container: 2
2023-06-30 20:51:59,548:INFO:PassiveAggressiveRegressor(random_state=6820)
2023-06-30 20:51:59,548:INFO:create_model() successfully completed......................................
2023-06-30 20:51:59,732:INFO:SubProcess create_model() end ==================================
2023-06-30 20:51:59,732:INFO:Creating metrics dataframe
2023-06-30 20:51:59,756:INFO:Initializing Huber Regressor
2023-06-30 20:51:59,756:INFO:Total runtime is 0.6100960254669189 minutes
2023-06-30 20:51:59,764:INFO:SubProcess create_model() called ==================================
2023-06-30 20:51:59,764:INFO:Initializing create_model()
2023-06-30 20:51:59,764:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7D52F1C0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 20:51:59,764:INFO:Checking exceptions
2023-06-30 20:51:59,764:INFO:Importing libraries
2023-06-30 20:51:59,764:INFO:Copying training dataset
2023-06-30 20:51:59,780:INFO:Defining folds
2023-06-30 20:51:59,780:INFO:Declaring metric variables
2023-06-30 20:51:59,788:INFO:Importing untrained model
2023-06-30 20:51:59,796:INFO:Huber Regressor Imported successfully
2023-06-30 20:51:59,812:INFO:Starting cross validation
2023-06-30 20:51:59,820:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 20:52:00,396:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-30 20:52:00,396:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-30 20:52:00,412:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-30 20:52:00,420:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-30 20:52:01,725:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-30 20:52:01,781:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-30 20:52:01,821:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-30 20:52:01,845:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-30 20:52:02,853:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-30 20:52:02,861:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-30 20:52:03,237:INFO:Calculating mean and std
2023-06-30 20:52:03,237:INFO:Creating metrics dataframe
2023-06-30 20:52:03,358:INFO:Uploading results into container
2023-06-30 20:52:03,358:INFO:Uploading model into container now
2023-06-30 20:52:03,358:INFO:_master_model_container: 10
2023-06-30 20:52:03,358:INFO:_display_container: 2
2023-06-30 20:52:03,358:INFO:HuberRegressor()
2023-06-30 20:52:03,365:INFO:create_model() successfully completed......................................
2023-06-30 20:52:03,542:INFO:SubProcess create_model() end ==================================
2023-06-30 20:52:03,542:INFO:Creating metrics dataframe
2023-06-30 20:52:03,566:INFO:Initializing K Neighbors Regressor
2023-06-30 20:52:03,566:INFO:Total runtime is 0.67358261346817 minutes
2023-06-30 20:52:03,582:INFO:SubProcess create_model() called ==================================
2023-06-30 20:52:03,582:INFO:Initializing create_model()
2023-06-30 20:52:03,582:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7D52F1C0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 20:52:03,582:INFO:Checking exceptions
2023-06-30 20:52:03,582:INFO:Importing libraries
2023-06-30 20:52:03,582:INFO:Copying training dataset
2023-06-30 20:52:03,598:INFO:Defining folds
2023-06-30 20:52:03,598:INFO:Declaring metric variables
2023-06-30 20:52:03,606:INFO:Importing untrained model
2023-06-30 20:52:03,614:INFO:K Neighbors Regressor Imported successfully
2023-06-30 20:52:03,638:INFO:Starting cross validation
2023-06-30 20:52:03,638:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 20:52:06,294:INFO:Calculating mean and std
2023-06-30 20:52:06,294:INFO:Creating metrics dataframe
2023-06-30 20:52:06,414:INFO:Uploading results into container
2023-06-30 20:52:06,414:INFO:Uploading model into container now
2023-06-30 20:52:06,422:INFO:_master_model_container: 11
2023-06-30 20:52:06,422:INFO:_display_container: 2
2023-06-30 20:52:06,422:INFO:KNeighborsRegressor(n_jobs=-1)
2023-06-30 20:52:06,422:INFO:create_model() successfully completed......................................
2023-06-30 20:52:06,606:INFO:SubProcess create_model() end ==================================
2023-06-30 20:52:06,606:INFO:Creating metrics dataframe
2023-06-30 20:52:06,630:INFO:Initializing Decision Tree Regressor
2023-06-30 20:52:06,630:INFO:Total runtime is 0.7246617277463276 minutes
2023-06-30 20:52:06,638:INFO:SubProcess create_model() called ==================================
2023-06-30 20:52:06,638:INFO:Initializing create_model()
2023-06-30 20:52:06,638:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7D52F1C0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 20:52:06,638:INFO:Checking exceptions
2023-06-30 20:52:06,638:INFO:Importing libraries
2023-06-30 20:52:06,646:INFO:Copying training dataset
2023-06-30 20:52:06,654:INFO:Defining folds
2023-06-30 20:52:06,654:INFO:Declaring metric variables
2023-06-30 20:52:06,662:INFO:Importing untrained model
2023-06-30 20:52:06,670:INFO:Decision Tree Regressor Imported successfully
2023-06-30 20:52:06,686:INFO:Starting cross validation
2023-06-30 20:52:06,694:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 20:52:09,454:INFO:Calculating mean and std
2023-06-30 20:52:09,454:INFO:Creating metrics dataframe
2023-06-30 20:52:09,590:INFO:Uploading results into container
2023-06-30 20:52:09,590:INFO:Uploading model into container now
2023-06-30 20:52:09,598:INFO:_master_model_container: 12
2023-06-30 20:52:09,598:INFO:_display_container: 2
2023-06-30 20:52:09,598:INFO:DecisionTreeRegressor(random_state=6820)
2023-06-30 20:52:09,598:INFO:create_model() successfully completed......................................
2023-06-30 20:52:09,782:INFO:SubProcess create_model() end ==================================
2023-06-30 20:52:09,782:INFO:Creating metrics dataframe
2023-06-30 20:52:09,798:INFO:Initializing Random Forest Regressor
2023-06-30 20:52:09,798:INFO:Total runtime is 0.7774531722068786 minutes
2023-06-30 20:52:09,814:INFO:SubProcess create_model() called ==================================
2023-06-30 20:52:09,814:INFO:Initializing create_model()
2023-06-30 20:52:09,814:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7D52F1C0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 20:52:09,814:INFO:Checking exceptions
2023-06-30 20:52:09,814:INFO:Importing libraries
2023-06-30 20:52:09,814:INFO:Copying training dataset
2023-06-30 20:52:09,822:INFO:Defining folds
2023-06-30 20:52:09,822:INFO:Declaring metric variables
2023-06-30 20:52:09,830:INFO:Importing untrained model
2023-06-30 20:52:09,846:INFO:Random Forest Regressor Imported successfully
2023-06-30 20:52:09,862:INFO:Starting cross validation
2023-06-30 20:52:09,862:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 20:52:18,295:INFO:Calculating mean and std
2023-06-30 20:52:18,295:INFO:Creating metrics dataframe
2023-06-30 20:52:18,440:INFO:Uploading results into container
2023-06-30 20:52:18,440:INFO:Uploading model into container now
2023-06-30 20:52:18,440:INFO:_master_model_container: 13
2023-06-30 20:52:18,440:INFO:_display_container: 2
2023-06-30 20:52:18,440:INFO:RandomForestRegressor(n_jobs=-1, random_state=6820)
2023-06-30 20:52:18,440:INFO:create_model() successfully completed......................................
2023-06-30 20:52:18,624:INFO:SubProcess create_model() end ==================================
2023-06-30 20:52:18,624:INFO:Creating metrics dataframe
2023-06-30 20:52:18,648:INFO:Initializing Extra Trees Regressor
2023-06-30 20:52:18,648:INFO:Total runtime is 0.9249484221140543 minutes
2023-06-30 20:52:18,656:INFO:SubProcess create_model() called ==================================
2023-06-30 20:52:18,656:INFO:Initializing create_model()
2023-06-30 20:52:18,656:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7D52F1C0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 20:52:18,656:INFO:Checking exceptions
2023-06-30 20:52:18,656:INFO:Importing libraries
2023-06-30 20:52:18,656:INFO:Copying training dataset
2023-06-30 20:52:18,664:INFO:Defining folds
2023-06-30 20:52:18,664:INFO:Declaring metric variables
2023-06-30 20:52:18,680:INFO:Importing untrained model
2023-06-30 20:52:18,688:INFO:Extra Trees Regressor Imported successfully
2023-06-30 20:52:18,704:INFO:Starting cross validation
2023-06-30 20:52:18,712:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 20:52:22,473:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:231: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_transformer = self._memory_fit(

2023-06-30 20:52:23,065:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:52:23,241:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:52:25,800:INFO:Calculating mean and std
2023-06-30 20:52:25,800:INFO:Creating metrics dataframe
2023-06-30 20:52:25,946:INFO:Uploading results into container
2023-06-30 20:52:25,954:INFO:Uploading model into container now
2023-06-30 20:52:25,954:INFO:_master_model_container: 14
2023-06-30 20:52:25,954:INFO:_display_container: 2
2023-06-30 20:52:25,954:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=6820)
2023-06-30 20:52:25,954:INFO:create_model() successfully completed......................................
2023-06-30 20:52:26,131:INFO:SubProcess create_model() end ==================================
2023-06-30 20:52:26,131:INFO:Creating metrics dataframe
2023-06-30 20:52:26,155:INFO:Initializing AdaBoost Regressor
2023-06-30 20:52:26,155:INFO:Total runtime is 1.0500695983568826 minutes
2023-06-30 20:52:26,171:INFO:SubProcess create_model() called ==================================
2023-06-30 20:52:26,171:INFO:Initializing create_model()
2023-06-30 20:52:26,171:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7D52F1C0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 20:52:26,171:INFO:Checking exceptions
2023-06-30 20:52:26,171:INFO:Importing libraries
2023-06-30 20:52:26,171:INFO:Copying training dataset
2023-06-30 20:52:26,187:INFO:Defining folds
2023-06-30 20:52:26,187:INFO:Declaring metric variables
2023-06-30 20:52:26,195:INFO:Importing untrained model
2023-06-30 20:52:26,211:INFO:AdaBoost Regressor Imported successfully
2023-06-30 20:52:26,227:INFO:Starting cross validation
2023-06-30 20:52:26,227:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 20:52:28,426:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:52:28,426:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:52:31,761:INFO:Calculating mean and std
2023-06-30 20:52:31,769:INFO:Creating metrics dataframe
2023-06-30 20:52:31,964:INFO:Uploading results into container
2023-06-30 20:52:31,964:INFO:Uploading model into container now
2023-06-30 20:52:31,964:INFO:_master_model_container: 15
2023-06-30 20:52:31,964:INFO:_display_container: 2
2023-06-30 20:52:31,964:INFO:AdaBoostRegressor(random_state=6820)
2023-06-30 20:52:31,964:INFO:create_model() successfully completed......................................
2023-06-30 20:52:32,159:INFO:SubProcess create_model() end ==================================
2023-06-30 20:52:32,159:INFO:Creating metrics dataframe
2023-06-30 20:52:32,191:INFO:Initializing Gradient Boosting Regressor
2023-06-30 20:52:32,191:INFO:Total runtime is 1.1506744623184204 minutes
2023-06-30 20:52:32,197:INFO:SubProcess create_model() called ==================================
2023-06-30 20:52:32,197:INFO:Initializing create_model()
2023-06-30 20:52:32,197:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7D52F1C0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 20:52:32,197:INFO:Checking exceptions
2023-06-30 20:52:32,197:INFO:Importing libraries
2023-06-30 20:52:32,197:INFO:Copying training dataset
2023-06-30 20:52:32,213:INFO:Defining folds
2023-06-30 20:52:32,213:INFO:Declaring metric variables
2023-06-30 20:52:32,221:INFO:Importing untrained model
2023-06-30 20:52:32,229:INFO:Gradient Boosting Regressor Imported successfully
2023-06-30 20:52:32,245:INFO:Starting cross validation
2023-06-30 20:52:32,253:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 20:52:37,949:INFO:Calculating mean and std
2023-06-30 20:52:37,949:INFO:Creating metrics dataframe
2023-06-30 20:52:38,160:INFO:Uploading results into container
2023-06-30 20:52:38,160:INFO:Uploading model into container now
2023-06-30 20:52:38,160:INFO:_master_model_container: 16
2023-06-30 20:52:38,160:INFO:_display_container: 2
2023-06-30 20:52:38,160:INFO:GradientBoostingRegressor(random_state=6820)
2023-06-30 20:52:38,160:INFO:create_model() successfully completed......................................
2023-06-30 20:52:38,360:INFO:SubProcess create_model() end ==================================
2023-06-30 20:52:38,360:INFO:Creating metrics dataframe
2023-06-30 20:52:38,392:INFO:Initializing Light Gradient Boosting Machine
2023-06-30 20:52:38,392:INFO:Total runtime is 1.2540313760439554 minutes
2023-06-30 20:52:38,400:INFO:SubProcess create_model() called ==================================
2023-06-30 20:52:38,400:INFO:Initializing create_model()
2023-06-30 20:52:38,400:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7D52F1C0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 20:52:38,400:INFO:Checking exceptions
2023-06-30 20:52:38,400:INFO:Importing libraries
2023-06-30 20:52:38,408:INFO:Copying training dataset
2023-06-30 20:52:38,416:INFO:Defining folds
2023-06-30 20:52:38,416:INFO:Declaring metric variables
2023-06-30 20:52:38,432:INFO:Importing untrained model
2023-06-30 20:52:38,440:INFO:Light Gradient Boosting Machine Imported successfully
2023-06-30 20:52:38,464:INFO:Starting cross validation
2023-06-30 20:52:38,464:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 20:52:43,183:INFO:Calculating mean and std
2023-06-30 20:52:43,191:INFO:Creating metrics dataframe
2023-06-30 20:52:43,409:INFO:Uploading results into container
2023-06-30 20:52:43,409:INFO:Uploading model into container now
2023-06-30 20:52:43,409:INFO:_master_model_container: 17
2023-06-30 20:52:43,409:INFO:_display_container: 2
2023-06-30 20:52:43,409:INFO:LGBMRegressor(random_state=6820)
2023-06-30 20:52:43,409:INFO:create_model() successfully completed......................................
2023-06-30 20:52:43,603:INFO:SubProcess create_model() end ==================================
2023-06-30 20:52:43,603:INFO:Creating metrics dataframe
2023-06-30 20:52:43,635:INFO:Initializing Dummy Regressor
2023-06-30 20:52:43,635:INFO:Total runtime is 1.341405463218689 minutes
2023-06-30 20:52:43,643:INFO:SubProcess create_model() called ==================================
2023-06-30 20:52:43,643:INFO:Initializing create_model()
2023-06-30 20:52:43,643:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C7D52F1C0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 20:52:43,643:INFO:Checking exceptions
2023-06-30 20:52:43,643:INFO:Importing libraries
2023-06-30 20:52:43,643:INFO:Copying training dataset
2023-06-30 20:52:43,659:INFO:Defining folds
2023-06-30 20:52:43,659:INFO:Declaring metric variables
2023-06-30 20:52:43,667:INFO:Importing untrained model
2023-06-30 20:52:43,675:INFO:Dummy Regressor Imported successfully
2023-06-30 20:52:43,691:INFO:Starting cross validation
2023-06-30 20:52:43,699:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 20:52:46,630:INFO:Calculating mean and std
2023-06-30 20:52:46,630:INFO:Creating metrics dataframe
2023-06-30 20:52:46,860:INFO:Uploading results into container
2023-06-30 20:52:46,860:INFO:Uploading model into container now
2023-06-30 20:52:46,860:INFO:_master_model_container: 18
2023-06-30 20:52:46,860:INFO:_display_container: 2
2023-06-30 20:52:46,860:INFO:DummyRegressor()
2023-06-30 20:52:46,860:INFO:create_model() successfully completed......................................
2023-06-30 20:52:47,045:INFO:SubProcess create_model() end ==================================
2023-06-30 20:52:47,053:INFO:Creating metrics dataframe
2023-06-30 20:52:47,109:INFO:Initializing create_model()
2023-06-30 20:52:47,109:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=GradientBoostingRegressor(random_state=6820), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-06-30 20:52:47,109:INFO:Checking exceptions
2023-06-30 20:52:47,117:INFO:Importing libraries
2023-06-30 20:52:47,117:INFO:Copying training dataset
2023-06-30 20:52:47,125:INFO:Defining folds
2023-06-30 20:52:47,125:INFO:Declaring metric variables
2023-06-30 20:52:47,125:INFO:Importing untrained model
2023-06-30 20:52:47,125:INFO:Declaring custom model
2023-06-30 20:52:47,133:INFO:Gradient Boosting Regressor Imported successfully
2023-06-30 20:52:47,133:INFO:Cross validation set to False
2023-06-30 20:52:47,133:INFO:Fitting Model
2023-06-30 20:52:48,246:INFO:GradientBoostingRegressor(random_state=6820)
2023-06-30 20:52:48,246:INFO:create_model() successfully completed......................................
2023-06-30 20:52:48,520:INFO:_master_model_container: 18
2023-06-30 20:52:48,520:INFO:_display_container: 2
2023-06-30 20:52:48,528:INFO:GradientBoostingRegressor(random_state=6820)
2023-06-30 20:52:48,528:INFO:compare_models() successfully completed......................................
2023-06-30 20:52:55,220:INFO:Initializing create_model()
2023-06-30 20:52:55,220:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=gbr, fold=10, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-06-30 20:52:55,220:INFO:Checking exceptions
2023-06-30 20:52:55,260:INFO:Importing libraries
2023-06-30 20:52:55,260:INFO:Copying training dataset
2023-06-30 20:52:55,276:INFO:Defining folds
2023-06-30 20:52:55,276:INFO:Declaring metric variables
2023-06-30 20:52:55,292:INFO:Importing untrained model
2023-06-30 20:52:55,308:INFO:Gradient Boosting Regressor Imported successfully
2023-06-30 20:52:55,324:INFO:Starting cross validation
2023-06-30 20:52:55,332:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 20:52:59,716:INFO:Calculating mean and std
2023-06-30 20:52:59,724:INFO:Creating metrics dataframe
2023-06-30 20:52:59,740:INFO:Finalizing model
2023-06-30 20:53:00,263:INFO:Uploading results into container
2023-06-30 20:53:00,263:INFO:Uploading model into container now
2023-06-30 20:53:00,287:INFO:_master_model_container: 19
2023-06-30 20:53:00,287:INFO:_display_container: 3
2023-06-30 20:53:00,287:INFO:GradientBoostingRegressor(random_state=6820)
2023-06-30 20:53:00,287:INFO:create_model() successfully completed......................................
2023-06-30 20:53:13,179:INFO:Initializing tune_model()
2023-06-30 20:53:13,179:INFO:tune_model(estimator=GradientBoostingRegressor(random_state=6820), fold=10, round=4, n_iter=30, custom_grid={'learning_rate': [0.01, 0.02, 0.05], 'max_depth': [1, 2, 3, 4, 5, 6, 7, 8], 'subsample': [0.4, 0.5, 0.6, 0.7, 0.8], 'n_estimators': [100, 200.3, 400, 500, 600]}, optimize=RMSE, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>)
2023-06-30 20:53:13,179:INFO:Checking exceptions
2023-06-30 20:53:13,219:INFO:Copying training dataset
2023-06-30 20:53:13,227:INFO:Checking base model
2023-06-30 20:53:13,227:INFO:Base model : Gradient Boosting Regressor
2023-06-30 20:53:13,243:INFO:Declaring metric variables
2023-06-30 20:53:13,275:INFO:Defining Hyperparameters
2023-06-30 20:53:13,555:INFO:custom_grid: {'actual_estimator__learning_rate': [0.01, 0.02, 0.05], 'actual_estimator__max_depth': [1, 2, 3, 4, 5, 6, 7, 8], 'actual_estimator__subsample': [0.4, 0.5, 0.6, 0.7, 0.8], 'actual_estimator__n_estimators': [100, 200.3, 400, 500, 600]}
2023-06-30 20:53:13,555:INFO:Tuning with n_jobs=-1
2023-06-30 20:53:13,555:INFO:Initializing RandomizedSearchCV
2023-06-30 20:53:19,383:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:53:19,431:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:53:19,495:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:53:19,567:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:53:21,274:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.98s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:53:21,314:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.97s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:53:21,610:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.08s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:53:21,626:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 1.01s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:53:27,244:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:53:27,252:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:53:27,380:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:53:27,461:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:53:28,404:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:53:28,556:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:53:28,564:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:53:33,177:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:53:33,844:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:53:33,860:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:53:34,965:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:53:35,029:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:53:36,879:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:53:37,895:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:53:38,183:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:53:38,863:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:53:38,919:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:53:39,920:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:53:42,905:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:53:44,626:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:53:44,906:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:53:56,848:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:54:00,615:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:54:07,877:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:54:09,953:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:54:13,369:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:54:15,298:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:54:18,012:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:54:18,020:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:54:19,236:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:54:19,260:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:54:20,861:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:54:21,574:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:54:21,974:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:54:22,046:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:54:22,790:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:54:23,271:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:54:23,327:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:54:25,024:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:54:25,592:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:54:26,008:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:54:26,008:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:54:26,104:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:54:26,881:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:54:27,225:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:54:27,281:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:54:37,509:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:54:37,965:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:54:42,984:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:54:46,633:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:54:47,018:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:54:47,970:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:54:48,362:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:54:54,021:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:54:54,485:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:54:54,589:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:54:55,086:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:54:55,366:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:54:55,790:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:55:00,365:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:55:00,653:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:55:00,941:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:55:01,758:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:55:05,449:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:55:06,745:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:55:06,945:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:55:20,646:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:55:21,997:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:55:22,247:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:55:22,503:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:55:22,639:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:55:23,548:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:55:23,829:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:55:28,613:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:55:29,007:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:55:29,796:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:55:30,149:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:55:30,302:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:55:30,503:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:55:31,318:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:55:31,813:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:55:36,965:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:55:37,780:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:55:38,892:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.93s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:55:39,452:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:56:04,138:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:56:04,682:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:56:05,683:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:56:08,965:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:56:12,510:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:56:13,300:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:56:37,590:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:56:37,958:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:56:38,662:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:56:38,726:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:56:39,031:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:56:40,487:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:56:43,952:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:56:44,920:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:56:47,114:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:56:48,034:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:56:48,082:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:56:51,396:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:56:52,284:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:56:52,316:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:56:55,477:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:56:55,493:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:02,812:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:57:03,252:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:57:03,949:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:04,181:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:57:04,613:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:05,317:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:57:05,797:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:06,782:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:11,871:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:57:12,512:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:57:13,184:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:13,464:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:57:13,696:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:14,633:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:57:14,913:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:15,881:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:22,348:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:57:22,644:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:57:23,740:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:23,996:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:24,749:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:57:25,870:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:57:26,022:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:27,150:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:27,991:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:57:28,231:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:57:29,112:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:29,512:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:29,984:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:57:31,273:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:57:31,313:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:32,529:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:33,514:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:57:33,770:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:57:34,651:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:35,051:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:35,355:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:57:36,571:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 20:57:36,843:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:37,892:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:52,374:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:378: FitFailedWarning: 
50 fits failed out of a total of 300.
The score on these train-test partitions for these parameters will be set to nan.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
50 fits failed with the following error:
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "c:\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "c:\Python310\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "c:\Python310\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "c:\Python310\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "c:\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "c:\Python310\lib\site-packages\sklearn\ensemble\_gb.py", line 420, in fit
    self._validate_params()
  File "c:\Python310\lib\site-packages\sklearn\base.py", line 600, in _validate_params
    validate_parameter_constraints(
  File "c:\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 97, in validate_parameter_constraints
    raise InvalidParameterError(
sklearn.utils._param_validation.InvalidParameterError: The 'n_estimators' parameter of GradientBoostingRegressor must be an int in the range [1, inf). Got 200.3 instead.

  warnings.warn(some_fits_failed_message, FitFailedWarning)

2023-06-30 20:57:52,390:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_search.py:952: UserWarning: One or more of the test scores are non-finite: [-5035.29784077 -4601.90105553 -4658.80967509 -6130.18407355
 -4706.81690876 -4763.35878613 -4500.09984355            nan
            nan -4803.17193712            nan -4849.86451314
 -4465.12540223 -4914.63302252 -4512.27866551 -6117.44861162
 -5924.63880473 -6116.47281089 -4537.36039221 -4541.21116329
 -4812.76775043 -4923.63289187            nan -4485.01798082
 -4475.68048387 -4970.5663086             nan -4520.34587627
 -4746.62661014 -4740.97527671]
  warnings.warn(

2023-06-30 20:57:53,014:INFO:best_params: {'actual_estimator__subsample': 0.8, 'actual_estimator__n_estimators': 400, 'actual_estimator__max_depth': 3, 'actual_estimator__learning_rate': 0.01}
2023-06-30 20:57:53,022:INFO:Hyperparameter search completed
2023-06-30 20:57:53,022:INFO:SubProcess create_model() called ==================================
2023-06-30 20:57:53,022:INFO:Initializing create_model()
2023-06-30 20:57:53,022:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=GradientBoostingRegressor(random_state=6820), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028C79EDAE00>, model_only=True, return_train_score=False, kwargs={'subsample': 0.8, 'n_estimators': 400, 'max_depth': 3, 'learning_rate': 0.01})
2023-06-30 20:57:53,022:INFO:Checking exceptions
2023-06-30 20:57:53,022:INFO:Importing libraries
2023-06-30 20:57:53,022:INFO:Copying training dataset
2023-06-30 20:57:53,030:INFO:Defining folds
2023-06-30 20:57:53,030:INFO:Declaring metric variables
2023-06-30 20:57:53,038:INFO:Importing untrained model
2023-06-30 20:57:53,038:INFO:Declaring custom model
2023-06-30 20:57:53,054:INFO:Gradient Boosting Regressor Imported successfully
2023-06-30 20:57:53,070:INFO:Starting cross validation
2023-06-30 20:57:53,070:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 20:57:54,662:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:54,738:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:54,792:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:54,912:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:57,164:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:57,260:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:57,485:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:57:57,654:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 20:58:01,348:INFO:Calculating mean and std
2023-06-30 20:58:01,356:INFO:Creating metrics dataframe
2023-06-30 20:58:01,372:INFO:Finalizing model
2023-06-30 20:58:04,811:INFO:Uploading results into container
2023-06-30 20:58:04,820:INFO:Uploading model into container now
2023-06-30 20:58:04,820:INFO:_master_model_container: 20
2023-06-30 20:58:04,820:INFO:_display_container: 4
2023-06-30 20:58:04,820:INFO:GradientBoostingRegressor(learning_rate=0.01, n_estimators=400,
                          random_state=6820, subsample=0.8)
2023-06-30 20:58:04,820:INFO:create_model() successfully completed......................................
2023-06-30 20:58:05,021:INFO:SubProcess create_model() end ==================================
2023-06-30 20:58:05,021:INFO:choose_better activated
2023-06-30 20:58:05,029:INFO:SubProcess create_model() called ==================================
2023-06-30 20:58:05,029:INFO:Initializing create_model()
2023-06-30 20:58:05,029:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=GradientBoostingRegressor(random_state=6820), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-06-30 20:58:05,029:INFO:Checking exceptions
2023-06-30 20:58:05,037:INFO:Importing libraries
2023-06-30 20:58:05,037:INFO:Copying training dataset
2023-06-30 20:58:05,045:INFO:Defining folds
2023-06-30 20:58:05,045:INFO:Declaring metric variables
2023-06-30 20:58:05,045:INFO:Importing untrained model
2023-06-30 20:58:05,045:INFO:Declaring custom model
2023-06-30 20:58:05,045:INFO:Gradient Boosting Regressor Imported successfully
2023-06-30 20:58:05,045:INFO:Starting cross validation
2023-06-30 20:58:05,053:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 20:58:11,997:INFO:Calculating mean and std
2023-06-30 20:58:11,997:INFO:Creating metrics dataframe
2023-06-30 20:58:11,997:INFO:Finalizing model
2023-06-30 20:58:12,985:INFO:Uploading results into container
2023-06-30 20:58:12,985:INFO:Uploading model into container now
2023-06-30 20:58:12,985:INFO:_master_model_container: 21
2023-06-30 20:58:12,985:INFO:_display_container: 5
2023-06-30 20:58:12,985:INFO:GradientBoostingRegressor(random_state=6820)
2023-06-30 20:58:12,985:INFO:create_model() successfully completed......................................
2023-06-30 20:58:13,172:INFO:SubProcess create_model() end ==================================
2023-06-30 20:58:13,180:INFO:GradientBoostingRegressor(random_state=6820) result for RMSE is 4663.0356
2023-06-30 20:58:13,180:INFO:GradientBoostingRegressor(learning_rate=0.01, n_estimators=400,
                          random_state=6820, subsample=0.8) result for RMSE is 4465.1254
2023-06-30 20:58:13,180:INFO:GradientBoostingRegressor(learning_rate=0.01, n_estimators=400,
                          random_state=6820, subsample=0.8) is best model
2023-06-30 20:58:13,180:INFO:choose_better completed
2023-06-30 20:58:13,204:INFO:_master_model_container: 21
2023-06-30 20:58:13,204:INFO:_display_container: 4
2023-06-30 20:58:13,204:INFO:GradientBoostingRegressor(learning_rate=0.01, n_estimators=400,
                          random_state=6820, subsample=0.8)
2023-06-30 20:58:13,204:INFO:tune_model() successfully completed......................................
2023-06-30 20:58:14,096:INFO:Initializing predict_model()
2023-06-30 20:58:14,096:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=GradientBoostingRegressor(random_state=6820), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000028C7F668820>)
2023-06-30 20:58:14,096:INFO:Checking exceptions
2023-06-30 20:58:14,096:INFO:Preloading libraries
2023-06-30 20:58:14,713:INFO:Initializing evaluate_model()
2023-06-30 20:58:14,713:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=GradientBoostingRegressor(random_state=6820), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-06-30 20:58:14,879:INFO:Initializing plot_model()
2023-06-30 20:58:14,879:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=6820), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, system=True)
2023-06-30 20:58:14,879:INFO:Checking exceptions
2023-06-30 20:58:14,887:INFO:Preloading libraries
2023-06-30 20:58:14,911:INFO:Copying training dataset
2023-06-30 20:58:14,911:INFO:Plot type: pipeline
2023-06-30 20:58:16,181:INFO:Visual Rendered Successfully
2023-06-30 20:58:16,382:INFO:plot_model() successfully completed......................................
2023-06-30 20:58:16,486:INFO:Initializing finalize_model()
2023-06-30 20:58:16,486:INFO:finalize_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=GradientBoostingRegressor(random_state=6820), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2023-06-30 20:58:16,494:INFO:Finalizing GradientBoostingRegressor(random_state=6820)
2023-06-30 20:58:16,502:INFO:Initializing create_model()
2023-06-30 20:58:16,502:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000028C7F72BC70>, estimator=GradientBoostingRegressor(random_state=6820), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, kwargs={})
2023-06-30 20:58:16,502:INFO:Checking exceptions
2023-06-30 20:58:16,502:INFO:Importing libraries
2023-06-30 20:58:16,502:INFO:Copying training dataset
2023-06-30 20:58:16,502:INFO:Defining folds
2023-06-30 20:58:16,502:INFO:Declaring metric variables
2023-06-30 20:58:16,510:INFO:Importing untrained model
2023-06-30 20:58:16,510:INFO:Declaring custom model
2023-06-30 20:58:16,510:INFO:Gradient Boosting Regressor Imported successfully
2023-06-30 20:58:16,510:INFO:Cross validation set to False
2023-06-30 20:58:16,510:INFO:Fitting Model
2023-06-30 20:58:18,140:INFO:Pipeline(memory=FastMemory(location=C:\Users\acer\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWr...
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('bin_numeric_features',
                 TransformerWrapper(include=['age', 'bmi'],
                                    transformer=KBinsDiscretizer(encode='ordinal',
                                                                 strategy='kmeans'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('actual_estimator',
                 GradientBoostingRegressor(random_state=6820))])
2023-06-30 20:58:18,140:INFO:create_model() successfully completed......................................
2023-06-30 20:58:18,336:INFO:_master_model_container: 21
2023-06-30 20:58:18,336:INFO:_display_container: 5
2023-06-30 20:58:18,400:INFO:Pipeline(memory=FastMemory(location=C:\Users\acer\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWr...
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('bin_numeric_features',
                 TransformerWrapper(include=['age', 'bmi'],
                                    transformer=KBinsDiscretizer(encode='ordinal',
                                                                 strategy='kmeans'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('actual_estimator',
                 GradientBoostingRegressor(random_state=6820))])
2023-06-30 20:58:18,400:INFO:finalize_model() successfully completed......................................
2023-06-30 20:58:18,738:INFO:Initializing save_model()
2023-06-30 20:58:18,738:INFO:save_model(model=Pipeline(memory=FastMemory(location=C:\Users\acer\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWr...
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('bin_numeric_features',
                 TransformerWrapper(include=['age', 'bmi'],
                                    transformer=KBinsDiscretizer(encode='ordinal',
                                                                 strategy='kmeans'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('actual_estimator',
                 GradientBoostingRegressor(random_state=6820))]), model_name=Premimum-Prediction_model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\acer\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWr...
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('bin_numeric_features',
                 TransformerWrapper(include=['age', 'bmi'],
                                    transformer=KBinsDiscretizer(encode='ordinal',
                                                                 strategy='kmeans'))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2023-06-30 20:58:18,738:INFO:Adding model into prep_pipe
2023-06-30 20:58:18,738:WARNING:Only Model saved as it was a pipeline.
2023-06-30 20:58:18,762:INFO:Premimum-Prediction_model.pkl saved in current working directory
2023-06-30 20:58:18,836:INFO:Pipeline(memory=FastMemory(location=C:\Users\acer\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWr...
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('bin_numeric_features',
                 TransformerWrapper(include=['age', 'bmi'],
                                    transformer=KBinsDiscretizer(encode='ordinal',
                                                                 strategy='kmeans'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('actual_estimator',
                 GradientBoostingRegressor(random_state=6820))])
2023-06-30 20:58:18,836:INFO:save_model() successfully completed......................................
2023-06-30 21:01:03,375:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 21:01:03,375:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 21:01:03,375:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 21:01:03,375:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 21:01:08,561:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-06-30 21:01:19,140:INFO:Initializing load_model()
2023-06-30 21:01:19,140:INFO:load_model(model_name=Premimum-prediction_model, platform=None, authentication=None, verbose=True)
2023-06-30 21:02:12,283:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 21:02:12,283:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 21:02:12,283:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 21:02:12,283:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 21:02:13,627:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-06-30 21:02:16,284:INFO:Initializing load_model()
2023-06-30 21:02:16,300:INFO:load_model(model_name=Premimum-prediction_model, platform=None, authentication=None, verbose=True)
2023-06-30 21:02:51,866:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 21:02:51,866:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 21:02:51,874:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 21:02:51,874:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 21:02:53,219:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-06-30 21:02:54,783:INFO:Initializing load_model()
2023-06-30 21:02:54,783:INFO:load_model(model_name=Premimum-prediction_model, platform=None, authentication=None, verbose=True)
2023-06-30 21:04:31,397:INFO:Initializing load_model()
2023-06-30 21:04:31,397:INFO:load_model(model_name=Premimum-prediction_model, platform=None, authentication=None, verbose=True)
2023-06-30 21:08:19,581:INFO:Initializing load_model()
2023-06-30 21:08:19,582:INFO:load_model(model_name=Premimum-prediction_model, platform=None, authentication=None, verbose=True)
2023-06-30 21:08:21,125:INFO:Initializing load_model()
2023-06-30 21:08:21,125:INFO:load_model(model_name=Premimum-prediction_model, platform=None, authentication=None, verbose=True)
2023-06-30 21:08:25,649:INFO:Initializing load_model()
2023-06-30 21:08:25,650:INFO:load_model(model_name=Premimum-prediction_model, platform=None, authentication=None, verbose=True)
2023-06-30 21:08:39,366:INFO:Initializing load_model()
2023-06-30 21:08:39,366:INFO:load_model(model_name=Premimum-prediction_model, platform=None, authentication=None, verbose=True)
2023-06-30 21:08:43,004:INFO:Initializing load_model()
2023-06-30 21:08:43,005:INFO:load_model(model_name=Premimum-prediction_model, platform=None, authentication=None, verbose=True)
2023-06-30 21:08:47,884:INFO:Initializing load_model()
2023-06-30 21:08:47,884:INFO:load_model(model_name=Premimum-prediction_model, platform=None, authentication=None, verbose=True)
2023-06-30 21:08:50,755:INFO:Initializing load_model()
2023-06-30 21:08:50,755:INFO:load_model(model_name=Premimum-prediction_model, platform=None, authentication=None, verbose=True)
2023-06-30 21:08:51,362:INFO:Initializing predict_model()
2023-06-30 21:08:51,362:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x0000018B7BC537C0>, estimator=Pipeline(memory=FastMemory(location=C:\Users\acer\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWr...
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('bin_numeric_features',
                 TransformerWrapper(include=['age', 'bmi'],
                                    transformer=KBinsDiscretizer(encode='ordinal',
                                                                 strategy='kmeans'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('actual_estimator',
                 GradientBoostingRegressor(random_state=6820))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000018B7BE68AF0>)
2023-06-30 21:08:51,362:INFO:Checking exceptions
2023-06-30 21:08:51,362:INFO:Preloading libraries
2023-06-30 21:08:51,362:INFO:Set up data.
2023-06-30 21:08:51,365:INFO:Set up index.
