2023-06-30 09:37:20,140:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 09:37:20,140:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 09:37:20,140:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 09:37:20,140:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 09:37:23,396:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-06-30 09:37:30,513:INFO:PyCaret RegressionExperiment
2023-06-30 09:37:30,513:INFO:Logging name: reg-default-name
2023-06-30 09:37:30,513:INFO:ML Usecase: MLUsecase.REGRESSION
2023-06-30 09:37:30,513:INFO:version 3.0.3
2023-06-30 09:37:30,513:INFO:Initializing setup()
2023-06-30 09:37:30,514:INFO:self.USI: 75ec
2023-06-30 09:37:30,514:INFO:self._variable_keys: {'pipeline', 'X_train', 'y_test', 'exp_name_log', 'n_jobs_param', 'gpu_param', '_available_plots', '_ml_usecase', 'X_test', 'target_param', 'idx', 'seed', 'html_param', 'fold_generator', 'fold_groups_param', 'data', 'logging_param', 'log_plots_param', 'fold_shuffle_param', 'transform_target_param', 'memory', 'y_train', 'USI', 'y', 'X', 'gpu_n_jobs_param', 'exp_id'}
2023-06-30 09:37:30,514:INFO:Checking environment
2023-06-30 09:37:30,514:INFO:python_version: 3.10.6
2023-06-30 09:37:30,514:INFO:python_build: ('tags/v3.10.6:9c7b4bd', 'Aug  1 2022 21:53:49')
2023-06-30 09:37:30,514:INFO:machine: AMD64
2023-06-30 09:37:30,514:INFO:platform: Windows-10-10.0.22621-SP0
2023-06-30 09:37:30,519:INFO:Memory: svmem(total=6378106880, available=1362210816, percent=78.6, used=5015896064, free=1362210816)
2023-06-30 09:37:30,519:INFO:Physical Core: 4
2023-06-30 09:37:30,519:INFO:Logical Core: 4
2023-06-30 09:37:30,519:INFO:Checking libraries
2023-06-30 09:37:30,519:INFO:System:
2023-06-30 09:37:30,519:INFO:    python: 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]
2023-06-30 09:37:30,519:INFO:executable: c:\Python310\python.exe
2023-06-30 09:37:30,519:INFO:   machine: Windows-10-10.0.22621-SP0
2023-06-30 09:37:30,519:INFO:PyCaret required dependencies:
2023-06-30 09:37:30,523:INFO:                 pip: 23.1.2
2023-06-30 09:37:30,523:INFO:          setuptools: 63.2.0
2023-06-30 09:37:30,523:INFO:             pycaret: 3.0.3
2023-06-30 09:37:30,523:INFO:             IPython: 8.14.0
2023-06-30 09:37:30,523:INFO:          ipywidgets: 8.0.6
2023-06-30 09:37:30,523:INFO:                tqdm: 4.65.0
2023-06-30 09:37:30,523:INFO:               numpy: 1.23.5
2023-06-30 09:37:30,523:INFO:              pandas: 1.5.3
2023-06-30 09:37:30,523:INFO:              jinja2: 3.1.2
2023-06-30 09:37:30,523:INFO:               scipy: 1.10.1
2023-06-30 09:37:30,524:INFO:              joblib: 1.2.0
2023-06-30 09:37:30,524:INFO:             sklearn: 1.2.2
2023-06-30 09:37:30,524:INFO:                pyod: 1.1.0
2023-06-30 09:37:30,524:INFO:            imblearn: 0.10.1
2023-06-30 09:37:30,524:INFO:   category_encoders: 2.6.1
2023-06-30 09:37:30,524:INFO:            lightgbm: 3.3.5
2023-06-30 09:37:30,524:INFO:               numba: 0.57.1
2023-06-30 09:37:30,524:INFO:            requests: 2.28.2
2023-06-30 09:37:30,524:INFO:          matplotlib: 3.7.1
2023-06-30 09:37:30,524:INFO:          scikitplot: 0.3.7
2023-06-30 09:37:30,524:INFO:         yellowbrick: 1.5
2023-06-30 09:37:30,524:INFO:              plotly: 5.15.0
2023-06-30 09:37:30,524:INFO:    plotly-resampler: Not installed
2023-06-30 09:37:30,524:INFO:             kaleido: 0.2.1
2023-06-30 09:37:30,525:INFO:           schemdraw: 0.15
2023-06-30 09:37:30,525:INFO:         statsmodels: 0.14.0
2023-06-30 09:37:30,525:INFO:              sktime: 0.20.0
2023-06-30 09:37:30,525:INFO:               tbats: 1.1.3
2023-06-30 09:37:30,525:INFO:            pmdarima: 2.0.3
2023-06-30 09:37:30,525:INFO:              psutil: 5.9.5
2023-06-30 09:37:30,525:INFO:          markupsafe: 2.1.2
2023-06-30 09:37:30,525:INFO:             pickle5: Not installed
2023-06-30 09:37:30,525:INFO:         cloudpickle: 2.2.1
2023-06-30 09:37:30,525:INFO:         deprecation: 2.1.0
2023-06-30 09:37:30,525:INFO:              xxhash: 3.2.0
2023-06-30 09:37:30,525:INFO:           wurlitzer: Not installed
2023-06-30 09:37:30,525:INFO:PyCaret optional dependencies:
2023-06-30 09:37:30,544:INFO:                shap: Not installed
2023-06-30 09:37:30,544:INFO:           interpret: Not installed
2023-06-30 09:37:30,545:INFO:                umap: Not installed
2023-06-30 09:37:30,545:INFO:    pandas_profiling: Not installed
2023-06-30 09:37:30,545:INFO:  explainerdashboard: Not installed
2023-06-30 09:37:30,545:INFO:             autoviz: Not installed
2023-06-30 09:37:30,545:INFO:           fairlearn: Not installed
2023-06-30 09:37:30,545:INFO:          deepchecks: Not installed
2023-06-30 09:37:30,545:INFO:             xgboost: Not installed
2023-06-30 09:37:30,545:INFO:            catboost: Not installed
2023-06-30 09:37:30,545:INFO:              kmodes: Not installed
2023-06-30 09:37:30,545:INFO:             mlxtend: Not installed
2023-06-30 09:37:30,545:INFO:       statsforecast: Not installed
2023-06-30 09:37:30,545:INFO:        tune_sklearn: Not installed
2023-06-30 09:37:30,545:INFO:                 ray: Not installed
2023-06-30 09:37:30,545:INFO:            hyperopt: Not installed
2023-06-30 09:37:30,545:INFO:              optuna: Not installed
2023-06-30 09:37:30,545:INFO:               skopt: Not installed
2023-06-30 09:37:30,545:INFO:              mlflow: Not installed
2023-06-30 09:37:30,545:INFO:              gradio: Not installed
2023-06-30 09:37:30,545:INFO:             fastapi: Not installed
2023-06-30 09:37:30,545:INFO:             uvicorn: Not installed
2023-06-30 09:37:30,546:INFO:              m2cgen: Not installed
2023-06-30 09:37:30,546:INFO:           evidently: Not installed
2023-06-30 09:37:30,546:INFO:               fugue: Not installed
2023-06-30 09:37:30,546:INFO:           streamlit: 1.20.0
2023-06-30 09:37:30,546:INFO:             prophet: Not installed
2023-06-30 09:37:30,546:INFO:None
2023-06-30 09:37:30,546:INFO:Set up data.
2023-06-30 09:37:30,555:INFO:Set up train/test split.
2023-06-30 09:37:30,561:INFO:Set up index.
2023-06-30 09:37:30,561:INFO:Set up folding strategy.
2023-06-30 09:37:30,561:INFO:Assigning column types.
2023-06-30 09:37:30,565:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-06-30 09:37:30,565:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-06-30 09:37:30,572:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-30 09:37:30,579:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 09:37:30,702:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:37:30,791:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 09:37:30,792:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:31,095:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:31,096:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-06-30 09:37:31,104:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-30 09:37:31,113:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 09:37:31,243:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:37:31,339:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 09:37:31,340:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:31,340:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:31,340:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-06-30 09:37:31,347:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-30 09:37:31,354:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 09:37:31,447:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:37:31,525:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 09:37:31,526:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:31,526:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:31,536:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-30 09:37:31,544:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 09:37:31,644:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:37:31,731:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 09:37:31,732:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:31,732:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:31,734:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-06-30 09:37:31,754:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 09:37:31,848:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:37:31,927:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 09:37:31,928:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:31,929:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:31,945:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 09:37:32,060:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:37:32,148:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 09:37:32,149:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:32,149:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:32,151:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-06-30 09:37:32,288:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:37:32,365:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 09:37:32,368:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:32,368:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:32,493:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:37:32,575:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 09:37:32,576:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:32,577:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:32,577:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-06-30 09:37:32,686:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:37:32,772:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:32,772:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:32,903:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:37:32,990:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:32,990:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:32,990:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-06-30 09:37:33,190:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:33,190:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:33,387:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:33,387:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:33,389:INFO:Preparing preprocessing pipeline...
2023-06-30 09:37:33,389:INFO:Set up simple imputation.
2023-06-30 09:37:33,393:INFO:Set up encoding of ordinal features.
2023-06-30 09:37:33,396:INFO:Set up encoding of categorical features.
2023-06-30 09:37:33,396:INFO:Set up feature normalization.
2023-06-30 09:37:33,574:INFO:Finished creating preprocessing pipeline.
2023-06-30 09:37:33,630:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\acer\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWr...
                                                                         'data_type': dtype('O'),
                                                                         'mapping': female    0
male      1
NaN      -1
dtype: int64},
                                                                        {'col': 'smoker',
                                                                         'data_type': dtype('O'),
                                                                         'mapping': no     0
yes    1
NaN   -1
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2023-06-30 09:37:33,630:INFO:Creating final display dataframe.
2023-06-30 09:37:34,098:INFO:Setup _display_container:                     Description             Value
0                    Session id              9085
1                        Target          expenses
2                   Target type        Regression
3           Original data shape         (1338, 7)
4        Transformed data shape        (1338, 10)
5   Transformed train set shape        (1070, 10)
6    Transformed test set shape         (268, 10)
7              Ordinal features                 2
8              Numeric features                 3
9          Categorical features                 3
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16                    Normalize              True
17             Normalize method            zscore
18               Fold Generator             KFold
19                  Fold Number                10
20                     CPU Jobs                -1
21                      Use GPU             False
22               Log Experiment             False
23              Experiment Name  reg-default-name
24                          USI              75ec
2023-06-30 09:37:34,320:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:34,321:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:34,545:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:34,545:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:37:34,546:INFO:setup() successfully completed in 4.48s...............
2023-06-30 09:39:13,032:INFO:PyCaret RegressionExperiment
2023-06-30 09:39:13,032:INFO:Logging name: reg-default-name
2023-06-30 09:39:13,033:INFO:ML Usecase: MLUsecase.REGRESSION
2023-06-30 09:39:13,033:INFO:version 3.0.3
2023-06-30 09:39:13,033:INFO:Initializing setup()
2023-06-30 09:39:13,033:INFO:self.USI: 2c1e
2023-06-30 09:39:13,033:INFO:self._variable_keys: {'pipeline', 'X_train', 'y_test', 'exp_name_log', 'n_jobs_param', 'gpu_param', '_available_plots', '_ml_usecase', 'X_test', 'target_param', 'idx', 'seed', 'html_param', 'fold_generator', 'fold_groups_param', 'data', 'logging_param', 'log_plots_param', 'fold_shuffle_param', 'transform_target_param', 'memory', 'y_train', 'USI', 'y', 'X', 'gpu_n_jobs_param', 'exp_id'}
2023-06-30 09:39:13,033:INFO:Checking environment
2023-06-30 09:39:13,033:INFO:python_version: 3.10.6
2023-06-30 09:39:13,033:INFO:python_build: ('tags/v3.10.6:9c7b4bd', 'Aug  1 2022 21:53:49')
2023-06-30 09:39:13,033:INFO:machine: AMD64
2023-06-30 09:39:13,033:INFO:platform: Windows-10-10.0.22621-SP0
2023-06-30 09:39:13,037:INFO:Memory: svmem(total=6378106880, available=1403645952, percent=78.0, used=4974460928, free=1403645952)
2023-06-30 09:39:13,037:INFO:Physical Core: 4
2023-06-30 09:39:13,037:INFO:Logical Core: 4
2023-06-30 09:39:13,038:INFO:Checking libraries
2023-06-30 09:39:13,038:INFO:System:
2023-06-30 09:39:13,038:INFO:    python: 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]
2023-06-30 09:39:13,038:INFO:executable: c:\Python310\python.exe
2023-06-30 09:39:13,038:INFO:   machine: Windows-10-10.0.22621-SP0
2023-06-30 09:39:13,038:INFO:PyCaret required dependencies:
2023-06-30 09:39:13,038:INFO:                 pip: 23.1.2
2023-06-30 09:39:13,038:INFO:          setuptools: 63.2.0
2023-06-30 09:39:13,038:INFO:             pycaret: 3.0.3
2023-06-30 09:39:13,038:INFO:             IPython: 8.14.0
2023-06-30 09:39:13,038:INFO:          ipywidgets: 8.0.6
2023-06-30 09:39:13,038:INFO:                tqdm: 4.65.0
2023-06-30 09:39:13,038:INFO:               numpy: 1.23.5
2023-06-30 09:39:13,039:INFO:              pandas: 1.5.3
2023-06-30 09:39:13,039:INFO:              jinja2: 3.1.2
2023-06-30 09:39:13,039:INFO:               scipy: 1.10.1
2023-06-30 09:39:13,039:INFO:              joblib: 1.2.0
2023-06-30 09:39:13,039:INFO:             sklearn: 1.2.2
2023-06-30 09:39:13,039:INFO:                pyod: 1.1.0
2023-06-30 09:39:13,039:INFO:            imblearn: 0.10.1
2023-06-30 09:39:13,039:INFO:   category_encoders: 2.6.1
2023-06-30 09:39:13,039:INFO:            lightgbm: 3.3.5
2023-06-30 09:39:13,039:INFO:               numba: 0.57.1
2023-06-30 09:39:13,039:INFO:            requests: 2.28.2
2023-06-30 09:39:13,039:INFO:          matplotlib: 3.7.1
2023-06-30 09:39:13,039:INFO:          scikitplot: 0.3.7
2023-06-30 09:39:13,039:INFO:         yellowbrick: 1.5
2023-06-30 09:39:13,039:INFO:              plotly: 5.15.0
2023-06-30 09:39:13,039:INFO:    plotly-resampler: Not installed
2023-06-30 09:39:13,039:INFO:             kaleido: 0.2.1
2023-06-30 09:39:13,040:INFO:           schemdraw: 0.15
2023-06-30 09:39:13,040:INFO:         statsmodels: 0.14.0
2023-06-30 09:39:13,040:INFO:              sktime: 0.20.0
2023-06-30 09:39:13,040:INFO:               tbats: 1.1.3
2023-06-30 09:39:13,040:INFO:            pmdarima: 2.0.3
2023-06-30 09:39:13,040:INFO:              psutil: 5.9.5
2023-06-30 09:39:13,040:INFO:          markupsafe: 2.1.2
2023-06-30 09:39:13,040:INFO:             pickle5: Not installed
2023-06-30 09:39:13,040:INFO:         cloudpickle: 2.2.1
2023-06-30 09:39:13,040:INFO:         deprecation: 2.1.0
2023-06-30 09:39:13,040:INFO:              xxhash: 3.2.0
2023-06-30 09:39:13,040:INFO:           wurlitzer: Not installed
2023-06-30 09:39:13,040:INFO:PyCaret optional dependencies:
2023-06-30 09:39:13,040:INFO:                shap: Not installed
2023-06-30 09:39:13,040:INFO:           interpret: Not installed
2023-06-30 09:39:13,040:INFO:                umap: Not installed
2023-06-30 09:39:13,040:INFO:    pandas_profiling: Not installed
2023-06-30 09:39:13,041:INFO:  explainerdashboard: Not installed
2023-06-30 09:39:13,041:INFO:             autoviz: Not installed
2023-06-30 09:39:13,041:INFO:           fairlearn: Not installed
2023-06-30 09:39:13,041:INFO:          deepchecks: Not installed
2023-06-30 09:39:13,041:INFO:             xgboost: Not installed
2023-06-30 09:39:13,041:INFO:            catboost: Not installed
2023-06-30 09:39:13,041:INFO:              kmodes: Not installed
2023-06-30 09:39:13,041:INFO:             mlxtend: Not installed
2023-06-30 09:39:13,041:INFO:       statsforecast: Not installed
2023-06-30 09:39:13,041:INFO:        tune_sklearn: Not installed
2023-06-30 09:39:13,041:INFO:                 ray: Not installed
2023-06-30 09:39:13,041:INFO:            hyperopt: Not installed
2023-06-30 09:39:13,041:INFO:              optuna: Not installed
2023-06-30 09:39:13,041:INFO:               skopt: Not installed
2023-06-30 09:39:13,041:INFO:              mlflow: Not installed
2023-06-30 09:39:13,041:INFO:              gradio: Not installed
2023-06-30 09:39:13,041:INFO:             fastapi: Not installed
2023-06-30 09:39:13,041:INFO:             uvicorn: Not installed
2023-06-30 09:39:13,041:INFO:              m2cgen: Not installed
2023-06-30 09:39:13,042:INFO:           evidently: Not installed
2023-06-30 09:39:13,042:INFO:               fugue: Not installed
2023-06-30 09:39:13,042:INFO:           streamlit: 1.20.0
2023-06-30 09:39:13,042:INFO:             prophet: Not installed
2023-06-30 09:39:13,042:INFO:None
2023-06-30 09:39:13,042:INFO:Set up data.
2023-06-30 09:39:13,051:INFO:Set up train/test split.
2023-06-30 09:39:13,057:INFO:Set up index.
2023-06-30 09:39:13,057:INFO:Set up folding strategy.
2023-06-30 09:39:13,057:INFO:Assigning column types.
2023-06-30 09:39:13,062:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-06-30 09:39:13,062:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-06-30 09:39:13,069:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-30 09:39:13,075:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 09:39:13,175:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:39:13,257:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 09:39:13,258:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:13,258:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:13,259:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-06-30 09:39:13,269:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-30 09:39:13,278:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 09:39:13,385:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:39:13,470:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 09:39:13,471:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:13,471:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:13,471:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-06-30 09:39:13,478:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-30 09:39:13,486:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 09:39:13,592:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:39:13,712:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 09:39:13,715:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:13,715:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:13,740:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-30 09:39:13,748:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 09:39:13,864:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:39:13,946:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 09:39:13,947:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:13,948:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:13,949:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-06-30 09:39:13,967:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 09:39:14,110:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:39:14,202:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 09:39:14,203:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:14,203:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:14,221:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 09:39:14,333:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:39:14,480:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 09:39:14,481:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:14,481:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:14,482:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-06-30 09:39:14,607:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:39:14,700:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 09:39:14,701:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:14,701:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:14,839:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:39:14,932:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 09:39:14,932:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:14,933:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:14,933:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-06-30 09:39:15,098:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:39:15,196:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:15,197:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:15,317:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:39:15,406:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:15,406:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:15,407:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-06-30 09:39:15,600:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:15,601:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:15,835:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:15,835:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:15,837:INFO:Preparing preprocessing pipeline...
2023-06-30 09:39:15,837:INFO:Set up simple imputation.
2023-06-30 09:39:15,841:INFO:Set up encoding of ordinal features.
2023-06-30 09:39:15,846:INFO:Set up encoding of categorical features.
2023-06-30 09:39:15,846:INFO:Set up feature normalization.
2023-06-30 09:39:15,970:INFO:Finished creating preprocessing pipeline.
2023-06-30 09:39:16,021:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\acer\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWr...
                                                                         'data_type': dtype('O'),
                                                                         'mapping': female    0
male      1
NaN      -1
dtype: int64},
                                                                        {'col': 'smoker',
                                                                         'data_type': dtype('O'),
                                                                         'mapping': no     0
yes    1
NaN   -1
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['region'],
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2023-06-30 09:39:16,021:INFO:Creating final display dataframe.
2023-06-30 09:39:16,408:INFO:Setup _display_container:                     Description             Value
0                    Session id              9085
1                        Target          expenses
2                   Target type        Regression
3           Original data shape         (1338, 7)
4        Transformed data shape        (1338, 10)
5   Transformed train set shape        (1070, 10)
6    Transformed test set shape         (268, 10)
7              Ordinal features                 2
8              Numeric features                 3
9          Categorical features                 3
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16                    Normalize              True
17             Normalize method            zscore
18               Fold Generator             KFold
19                  Fold Number                10
20                     CPU Jobs                -1
21                      Use GPU             False
22               Log Experiment             False
23              Experiment Name  reg-default-name
24                          USI              2c1e
2023-06-30 09:39:16,641:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:16,641:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:16,845:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:16,846:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:16,846:INFO:setup() successfully completed in 4.18s...............
2023-06-30 09:39:17,802:INFO:PyCaret RegressionExperiment
2023-06-30 09:39:17,802:INFO:Logging name: reg-default-name
2023-06-30 09:39:17,802:INFO:ML Usecase: MLUsecase.REGRESSION
2023-06-30 09:39:17,802:INFO:version 3.0.3
2023-06-30 09:39:17,803:INFO:Initializing setup()
2023-06-30 09:39:17,803:INFO:self.USI: f821
2023-06-30 09:39:17,803:INFO:self._variable_keys: {'pipeline', 'X_train', 'y_test', 'exp_name_log', 'n_jobs_param', 'gpu_param', '_available_plots', '_ml_usecase', 'X_test', 'target_param', 'idx', 'seed', 'html_param', 'fold_generator', 'fold_groups_param', 'data', 'logging_param', 'log_plots_param', 'fold_shuffle_param', 'transform_target_param', 'memory', 'y_train', 'USI', 'y', 'X', 'gpu_n_jobs_param', 'exp_id'}
2023-06-30 09:39:17,803:INFO:Checking environment
2023-06-30 09:39:17,803:INFO:python_version: 3.10.6
2023-06-30 09:39:17,803:INFO:python_build: ('tags/v3.10.6:9c7b4bd', 'Aug  1 2022 21:53:49')
2023-06-30 09:39:17,803:INFO:machine: AMD64
2023-06-30 09:39:17,803:INFO:platform: Windows-10-10.0.22621-SP0
2023-06-30 09:39:17,807:INFO:Memory: svmem(total=6378106880, available=1410609152, percent=77.9, used=4967497728, free=1410609152)
2023-06-30 09:39:17,807:INFO:Physical Core: 4
2023-06-30 09:39:17,807:INFO:Logical Core: 4
2023-06-30 09:39:17,807:INFO:Checking libraries
2023-06-30 09:39:17,808:INFO:System:
2023-06-30 09:39:17,808:INFO:    python: 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]
2023-06-30 09:39:17,808:INFO:executable: c:\Python310\python.exe
2023-06-30 09:39:17,808:INFO:   machine: Windows-10-10.0.22621-SP0
2023-06-30 09:39:17,808:INFO:PyCaret required dependencies:
2023-06-30 09:39:17,808:INFO:                 pip: 23.1.2
2023-06-30 09:39:17,808:INFO:          setuptools: 63.2.0
2023-06-30 09:39:17,808:INFO:             pycaret: 3.0.3
2023-06-30 09:39:17,808:INFO:             IPython: 8.14.0
2023-06-30 09:39:17,808:INFO:          ipywidgets: 8.0.6
2023-06-30 09:39:17,808:INFO:                tqdm: 4.65.0
2023-06-30 09:39:17,808:INFO:               numpy: 1.23.5
2023-06-30 09:39:17,808:INFO:              pandas: 1.5.3
2023-06-30 09:39:17,808:INFO:              jinja2: 3.1.2
2023-06-30 09:39:17,809:INFO:               scipy: 1.10.1
2023-06-30 09:39:17,809:INFO:              joblib: 1.2.0
2023-06-30 09:39:17,809:INFO:             sklearn: 1.2.2
2023-06-30 09:39:17,809:INFO:                pyod: 1.1.0
2023-06-30 09:39:17,809:INFO:            imblearn: 0.10.1
2023-06-30 09:39:17,809:INFO:   category_encoders: 2.6.1
2023-06-30 09:39:17,809:INFO:            lightgbm: 3.3.5
2023-06-30 09:39:17,809:INFO:               numba: 0.57.1
2023-06-30 09:39:17,809:INFO:            requests: 2.28.2
2023-06-30 09:39:17,809:INFO:          matplotlib: 3.7.1
2023-06-30 09:39:17,809:INFO:          scikitplot: 0.3.7
2023-06-30 09:39:17,809:INFO:         yellowbrick: 1.5
2023-06-30 09:39:17,809:INFO:              plotly: 5.15.0
2023-06-30 09:39:17,809:INFO:    plotly-resampler: Not installed
2023-06-30 09:39:17,809:INFO:             kaleido: 0.2.1
2023-06-30 09:39:17,809:INFO:           schemdraw: 0.15
2023-06-30 09:39:17,809:INFO:         statsmodels: 0.14.0
2023-06-30 09:39:17,809:INFO:              sktime: 0.20.0
2023-06-30 09:39:17,809:INFO:               tbats: 1.1.3
2023-06-30 09:39:17,810:INFO:            pmdarima: 2.0.3
2023-06-30 09:39:17,810:INFO:              psutil: 5.9.5
2023-06-30 09:39:17,810:INFO:          markupsafe: 2.1.2
2023-06-30 09:39:17,810:INFO:             pickle5: Not installed
2023-06-30 09:39:17,810:INFO:         cloudpickle: 2.2.1
2023-06-30 09:39:17,810:INFO:         deprecation: 2.1.0
2023-06-30 09:39:17,810:INFO:              xxhash: 3.2.0
2023-06-30 09:39:17,811:INFO:           wurlitzer: Not installed
2023-06-30 09:39:17,811:INFO:PyCaret optional dependencies:
2023-06-30 09:39:17,811:INFO:                shap: Not installed
2023-06-30 09:39:17,811:INFO:           interpret: Not installed
2023-06-30 09:39:17,811:INFO:                umap: Not installed
2023-06-30 09:39:17,811:INFO:    pandas_profiling: Not installed
2023-06-30 09:39:17,811:INFO:  explainerdashboard: Not installed
2023-06-30 09:39:17,812:INFO:             autoviz: Not installed
2023-06-30 09:39:17,812:INFO:           fairlearn: Not installed
2023-06-30 09:39:17,812:INFO:          deepchecks: Not installed
2023-06-30 09:39:17,812:INFO:             xgboost: Not installed
2023-06-30 09:39:17,812:INFO:            catboost: Not installed
2023-06-30 09:39:17,812:INFO:              kmodes: Not installed
2023-06-30 09:39:17,812:INFO:             mlxtend: Not installed
2023-06-30 09:39:17,812:INFO:       statsforecast: Not installed
2023-06-30 09:39:17,812:INFO:        tune_sklearn: Not installed
2023-06-30 09:39:17,813:INFO:                 ray: Not installed
2023-06-30 09:39:17,813:INFO:            hyperopt: Not installed
2023-06-30 09:39:17,813:INFO:              optuna: Not installed
2023-06-30 09:39:17,813:INFO:               skopt: Not installed
2023-06-30 09:39:17,813:INFO:              mlflow: Not installed
2023-06-30 09:39:17,813:INFO:              gradio: Not installed
2023-06-30 09:39:17,813:INFO:             fastapi: Not installed
2023-06-30 09:39:17,813:INFO:             uvicorn: Not installed
2023-06-30 09:39:17,814:INFO:              m2cgen: Not installed
2023-06-30 09:39:17,814:INFO:           evidently: Not installed
2023-06-30 09:39:17,814:INFO:               fugue: Not installed
2023-06-30 09:39:17,814:INFO:           streamlit: 1.20.0
2023-06-30 09:39:17,814:INFO:             prophet: Not installed
2023-06-30 09:39:17,814:INFO:None
2023-06-30 09:39:17,814:INFO:Set up data.
2023-06-30 09:39:17,824:INFO:Set up train/test split.
2023-06-30 09:39:17,834:INFO:Set up index.
2023-06-30 09:39:17,835:INFO:Set up folding strategy.
2023-06-30 09:39:17,835:INFO:Assigning column types.
2023-06-30 09:39:17,841:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-06-30 09:39:17,841:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-06-30 09:39:17,850:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-30 09:39:17,858:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 09:39:17,981:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:39:18,073:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 09:39:18,074:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:18,074:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:18,075:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-06-30 09:39:18,086:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-30 09:39:18,093:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 09:39:18,220:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:39:18,316:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 09:39:18,317:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:18,317:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:18,318:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-06-30 09:39:18,330:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-30 09:39:18,339:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 09:39:18,504:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:39:18,619:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 09:39:18,620:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:18,623:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:18,678:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-06-30 09:39:18,691:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 09:39:18,875:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:39:18,975:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 09:39:18,981:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:18,981:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:18,982:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-06-30 09:39:19,002:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 09:39:19,118:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:39:19,212:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 09:39:19,214:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:19,215:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:19,234:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-06-30 09:39:19,346:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:39:19,439:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 09:39:19,440:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:19,440:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:19,441:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-06-30 09:39:19,575:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:39:19,668:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 09:39:19,670:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:19,671:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:19,808:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:39:19,917:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-06-30 09:39:19,918:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:19,918:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:19,918:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-06-30 09:39:20,042:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:39:20,136:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:20,137:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:20,263:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-06-30 09:39:20,362:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:20,363:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:20,364:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-06-30 09:39:20,597:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:20,598:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:20,836:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:20,836:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:20,838:INFO:Preparing preprocessing pipeline...
2023-06-30 09:39:20,838:INFO:Set up simple imputation.
2023-06-30 09:39:20,841:INFO:Set up encoding of ordinal features.
2023-06-30 09:39:20,848:INFO:Set up encoding of categorical features.
2023-06-30 09:39:20,848:INFO:Set up polynomial features.
2023-06-30 09:39:20,849:INFO:Set up binning of numerical features.
2023-06-30 09:39:20,850:INFO:Set up feature normalization.
2023-06-30 09:39:21,234:INFO:Finished creating preprocessing pipeline.
2023-06-30 09:39:21,292:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\acer\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWr...
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('bin_numeric_features',
                 TransformerWrapper(include=['age', 'bmi'],
                                    transformer=KBinsDiscretizer(encode='ordinal',
                                                                 strategy='kmeans'))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))])
2023-06-30 09:39:21,293:INFO:Creating final display dataframe.
2023-06-30 09:39:21,952:INFO:Setup _display_container:                     Description             Value
0                    Session id              6820
1                        Target          expenses
2                   Target type        Regression
3           Original data shape         (1338, 7)
4        Transformed data shape        (1338, 55)
5   Transformed train set shape         (936, 55)
6    Transformed test set shape         (402, 55)
7              Ordinal features                 2
8              Numeric features                 3
9          Categorical features                 3
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16          Polynomial features              True
17            Polynomial degree                 2
18                    Normalize              True
19             Normalize method            zscore
20               Fold Generator             KFold
21                  Fold Number                10
22                     CPU Jobs                -1
23                      Use GPU             False
24               Log Experiment             False
25              Experiment Name  reg-default-name
26                          USI              f821
2023-06-30 09:39:22,171:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:22,325:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:22,512:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:22,513:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-06-30 09:39:22,514:INFO:setup() successfully completed in 5.09s...............
2023-06-30 09:39:22,575:INFO:Initializing compare_models()
2023-06-30 09:39:22,575:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, include=None, fold=None, round=4, cross_validation=True, sort=RMSE, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'RMSE', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-06-30 09:39:22,576:INFO:Checking exceptions
2023-06-30 09:39:22,579:INFO:Preparing display monitor
2023-06-30 09:39:22,622:INFO:Initializing Linear Regression
2023-06-30 09:39:22,622:INFO:Total runtime is 0.0 minutes
2023-06-30 09:39:22,629:INFO:SubProcess create_model() called ==================================
2023-06-30 09:39:22,630:INFO:Initializing create_model()
2023-06-30 09:39:22,630:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000245D8A272E0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 09:39:22,630:INFO:Checking exceptions
2023-06-30 09:39:22,630:INFO:Importing libraries
2023-06-30 09:39:22,631:INFO:Copying training dataset
2023-06-30 09:39:22,640:INFO:Defining folds
2023-06-30 09:39:22,640:INFO:Declaring metric variables
2023-06-30 09:39:22,648:INFO:Importing untrained model
2023-06-30 09:39:22,652:INFO:Linear Regression Imported successfully
2023-06-30 09:39:22,671:INFO:Starting cross validation
2023-06-30 09:39:22,748:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 09:39:35,656:INFO:Calculating mean and std
2023-06-30 09:39:35,657:INFO:Creating metrics dataframe
2023-06-30 09:39:36,202:INFO:Uploading results into container
2023-06-30 09:39:36,203:INFO:Uploading model into container now
2023-06-30 09:39:36,204:INFO:_master_model_container: 1
2023-06-30 09:39:36,205:INFO:_display_container: 2
2023-06-30 09:39:36,205:INFO:LinearRegression(n_jobs=-1)
2023-06-30 09:39:36,205:INFO:create_model() successfully completed......................................
2023-06-30 09:39:36,451:INFO:SubProcess create_model() end ==================================
2023-06-30 09:39:36,452:INFO:Creating metrics dataframe
2023-06-30 09:39:36,466:INFO:Initializing Lasso Regression
2023-06-30 09:39:36,466:INFO:Total runtime is 0.23073137601216634 minutes
2023-06-30 09:39:36,472:INFO:SubProcess create_model() called ==================================
2023-06-30 09:39:36,472:INFO:Initializing create_model()
2023-06-30 09:39:36,472:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000245D8A272E0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 09:39:36,472:INFO:Checking exceptions
2023-06-30 09:39:36,473:INFO:Importing libraries
2023-06-30 09:39:36,473:INFO:Copying training dataset
2023-06-30 09:39:36,482:INFO:Defining folds
2023-06-30 09:39:36,482:INFO:Declaring metric variables
2023-06-30 09:39:36,488:INFO:Importing untrained model
2023-06-30 09:39:36,497:INFO:Lasso Regression Imported successfully
2023-06-30 09:39:36,512:INFO:Starting cross validation
2023-06-30 09:39:36,514:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 09:39:37,257:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.408e+09, tolerance: 1.347e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-30 09:39:37,257:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.890e+09, tolerance: 1.264e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-30 09:39:37,260:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.768e+09, tolerance: 1.254e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-30 09:39:37,261:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.313e+09, tolerance: 1.291e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-30 09:39:38,754:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.819e+09, tolerance: 1.298e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-30 09:39:38,760:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.175e+09, tolerance: 1.271e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-30 09:39:38,833:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.334e+09, tolerance: 1.193e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-30 09:39:38,946:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.084e+09, tolerance: 1.313e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-30 09:39:39,924:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.935e+09, tolerance: 1.259e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-30 09:39:40,018:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.581e+09, tolerance: 1.283e+07
  model = cd_fast.enet_coordinate_descent(

2023-06-30 09:39:42,427:INFO:Calculating mean and std
2023-06-30 09:39:42,429:INFO:Creating metrics dataframe
2023-06-30 09:39:42,874:INFO:Uploading results into container
2023-06-30 09:39:42,877:INFO:Uploading model into container now
2023-06-30 09:39:42,878:INFO:_master_model_container: 2
2023-06-30 09:39:42,878:INFO:_display_container: 2
2023-06-30 09:39:42,878:INFO:Lasso(random_state=6820)
2023-06-30 09:39:42,879:INFO:create_model() successfully completed......................................
2023-06-30 09:39:42,990:INFO:SubProcess create_model() end ==================================
2023-06-30 09:39:42,990:INFO:Creating metrics dataframe
2023-06-30 09:39:43,005:INFO:Initializing Ridge Regression
2023-06-30 09:39:43,006:INFO:Total runtime is 0.3397321343421936 minutes
2023-06-30 09:39:43,012:INFO:SubProcess create_model() called ==================================
2023-06-30 09:39:43,013:INFO:Initializing create_model()
2023-06-30 09:39:43,013:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000245D8A272E0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 09:39:43,014:INFO:Checking exceptions
2023-06-30 09:39:43,014:INFO:Importing libraries
2023-06-30 09:39:43,014:INFO:Copying training dataset
2023-06-30 09:39:43,021:INFO:Defining folds
2023-06-30 09:39:43,021:INFO:Declaring metric variables
2023-06-30 09:39:43,028:INFO:Importing untrained model
2023-06-30 09:39:43,034:INFO:Ridge Regression Imported successfully
2023-06-30 09:39:43,047:INFO:Starting cross validation
2023-06-30 09:39:43,049:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 09:39:49,119:INFO:Calculating mean and std
2023-06-30 09:39:49,121:INFO:Creating metrics dataframe
2023-06-30 09:39:49,571:INFO:Uploading results into container
2023-06-30 09:39:49,572:INFO:Uploading model into container now
2023-06-30 09:39:49,573:INFO:_master_model_container: 3
2023-06-30 09:39:49,573:INFO:_display_container: 2
2023-06-30 09:39:49,575:INFO:Ridge(random_state=6820)
2023-06-30 09:39:49,575:INFO:create_model() successfully completed......................................
2023-06-30 09:39:49,690:INFO:SubProcess create_model() end ==================================
2023-06-30 09:39:49,690:INFO:Creating metrics dataframe
2023-06-30 09:39:49,707:INFO:Initializing Elastic Net
2023-06-30 09:39:49,707:INFO:Total runtime is 0.45141428311665854 minutes
2023-06-30 09:39:49,715:INFO:SubProcess create_model() called ==================================
2023-06-30 09:39:49,715:INFO:Initializing create_model()
2023-06-30 09:39:49,715:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000245D8A272E0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 09:39:49,715:INFO:Checking exceptions
2023-06-30 09:39:49,715:INFO:Importing libraries
2023-06-30 09:39:49,715:INFO:Copying training dataset
2023-06-30 09:39:49,722:INFO:Defining folds
2023-06-30 09:39:49,722:INFO:Declaring metric variables
2023-06-30 09:39:49,730:INFO:Importing untrained model
2023-06-30 09:39:49,736:INFO:Elastic Net Imported successfully
2023-06-30 09:39:49,748:INFO:Starting cross validation
2023-06-30 09:39:49,750:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 09:39:56,221:INFO:Calculating mean and std
2023-06-30 09:39:56,223:INFO:Creating metrics dataframe
2023-06-30 09:39:56,671:INFO:Uploading results into container
2023-06-30 09:39:56,672:INFO:Uploading model into container now
2023-06-30 09:39:56,673:INFO:_master_model_container: 4
2023-06-30 09:39:56,673:INFO:_display_container: 2
2023-06-30 09:39:56,673:INFO:ElasticNet(random_state=6820)
2023-06-30 09:39:56,674:INFO:create_model() successfully completed......................................
2023-06-30 09:39:56,785:INFO:SubProcess create_model() end ==================================
2023-06-30 09:39:56,785:INFO:Creating metrics dataframe
2023-06-30 09:39:56,802:INFO:Initializing Least Angle Regression
2023-06-30 09:39:56,802:INFO:Total runtime is 0.5696604569753011 minutes
2023-06-30 09:39:56,807:INFO:SubProcess create_model() called ==================================
2023-06-30 09:39:56,809:INFO:Initializing create_model()
2023-06-30 09:39:56,809:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000245D8A272E0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 09:39:56,809:INFO:Checking exceptions
2023-06-30 09:39:56,810:INFO:Importing libraries
2023-06-30 09:39:56,810:INFO:Copying training dataset
2023-06-30 09:39:56,816:INFO:Defining folds
2023-06-30 09:39:56,816:INFO:Declaring metric variables
2023-06-30 09:39:56,822:INFO:Importing untrained model
2023-06-30 09:39:56,829:INFO:Least Angle Regression Imported successfully
2023-06-30 09:39:56,838:INFO:Starting cross validation
2023-06-30 09:39:56,841:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 09:39:57,216:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=4.926e+01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,216:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.520e+02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,216:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=2.741e+03, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,216:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=4.267e+01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,217:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=1.889e+03, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,217:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=1.403e+03, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,217:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=8.132e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,217:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=3.163e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,219:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=9.019e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,219:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=4.965e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,219:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.393e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,219:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=1.914e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,219:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=5.294e+01, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,221:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.023e+02, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,221:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=8.610e+01, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,222:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=6.801e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,223:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=4.352e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,223:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=2.731e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,223:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=7.578e+00, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,226:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=6.788e+01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,228:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=6.867e+01, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,229:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=6.591e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,229:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=6.084e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,229:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=4.371e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,229:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=2.418e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,229:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=1.038e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:57,229:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=2.193e+00, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,437:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.435e+02, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,438:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.502e+02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,438:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=6.443e+01, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,439:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=4.750e+01, with an active set of 25 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,443:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.133e+01, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,444:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.201e+01, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,446:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.240e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,446:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.174e+01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,448:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=4.797e+00, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,448:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=3.236e+00, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,449:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.140e+00, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,449:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=2.102e+00, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,450:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.235e+00, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,450:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=6.907e-01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,538:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=1.358e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,538:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=1.325e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,539:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=9.719e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,541:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=9.667e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,542:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=5.427e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,543:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=3.120e+01, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,682:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=5.307e+02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,683:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=5.278e+02, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,685:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=4.341e+03, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,685:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=1.033e+03, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,686:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=7.522e+02, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,689:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=6.247e+01, with an active set of 32 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,709:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=6.934e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,713:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=2.590e+03, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,713:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=1.538e+03, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,713:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=9.552e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,714:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=3.848e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,714:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.924e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,714:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.603e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:58,714:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=6.116e+01, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:59,685:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=5.109e+01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:59,687:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=4.002e+01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:59,692:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=5.970e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:59,693:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=5.465e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:59,693:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=5.251e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:59,694:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=3.215e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:59,694:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=2.218e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:59,694:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=1.308e+01, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:59,766:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.714e+01, with an active set of 34 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:59,767:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=4.910e+01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:59,770:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=4.847e+03, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:59,770:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=3.201e+03, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:59,770:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=1.866e+03, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:39:59,771:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=1.180e+03, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:40:01,883:INFO:Calculating mean and std
2023-06-30 09:40:01,885:INFO:Creating metrics dataframe
2023-06-30 09:40:02,335:INFO:Uploading results into container
2023-06-30 09:40:02,336:INFO:Uploading model into container now
2023-06-30 09:40:02,337:INFO:_master_model_container: 5
2023-06-30 09:40:02,337:INFO:_display_container: 2
2023-06-30 09:40:02,337:INFO:Lars(random_state=6820)
2023-06-30 09:40:02,337:INFO:create_model() successfully completed......................................
2023-06-30 09:40:02,446:INFO:SubProcess create_model() end ==================================
2023-06-30 09:40:02,447:INFO:Creating metrics dataframe
2023-06-30 09:40:02,460:INFO:Initializing Lasso Least Angle Regression
2023-06-30 09:40:02,460:INFO:Total runtime is 0.6639630715052287 minutes
2023-06-30 09:40:02,463:INFO:SubProcess create_model() called ==================================
2023-06-30 09:40:02,464:INFO:Initializing create_model()
2023-06-30 09:40:02,464:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000245D8A272E0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 09:40:02,465:INFO:Checking exceptions
2023-06-30 09:40:02,465:INFO:Importing libraries
2023-06-30 09:40:02,465:INFO:Copying training dataset
2023-06-30 09:40:02,472:INFO:Defining folds
2023-06-30 09:40:02,472:INFO:Declaring metric variables
2023-06-30 09:40:02,479:INFO:Importing untrained model
2023-06-30 09:40:02,484:INFO:Lasso Least Angle Regression Imported successfully
2023-06-30 09:40:02,494:INFO:Starting cross validation
2023-06-30 09:40:02,497:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 09:40:02,816:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.678e+01, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:40:02,822:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=3.647e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:40:02,827:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=2.725e+02, previous alpha=2.605e+02, with an active set of 11 regressors.
  warnings.warn(

2023-06-30 09:40:02,833:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=3.535e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:40:02,835:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.426e+02, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:40:02,835:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 56 iterations, alpha=3.144e+00, previous alpha=3.142e+00, with an active set of 37 regressors.
  warnings.warn(

2023-06-30 09:40:02,835:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.262e+02, with an active set of 15 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:40:02,837:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=5.063e+01, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:40:02,838:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 27 iterations, alpha=6.094e+01, previous alpha=5.063e+01, with an active set of 22 regressors.
  warnings.warn(

2023-06-30 09:40:02,849:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=1.705e+02, previous alpha=1.654e+02, with an active set of 15 regressors.
  warnings.warn(

2023-06-30 09:40:04,047:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 14 iterations, alpha=1.771e+02, previous alpha=1.751e+02, with an active set of 15 regressors.
  warnings.warn(

2023-06-30 09:40:04,120:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.913e+01, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:40:04,121:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 35 iterations, i.e. alpha=1.881e+01, with an active set of 29 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:40:04,145:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=1.577e+01, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:40:04,147:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 38 iterations, alpha=1.342e+01, previous alpha=1.104e+01, with an active set of 31 regressors.
  warnings.warn(

2023-06-30 09:40:04,165:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 11 iterations, alpha=2.954e+02, previous alpha=2.821e+02, with an active set of 12 regressors.
  warnings.warn(

2023-06-30 09:40:04,261:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=3.328e+00, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:40:04,262:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=3.278e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:40:04,264:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=2.553e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:40:04,264:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=1.616e+00, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:40:04,266:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 62 iterations, alpha=2.553e+00, previous alpha=1.453e+00, with an active set of 39 regressors.
  warnings.warn(

2023-06-30 09:40:05,360:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=1.645e+01, with an active set of 26 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:40:05,363:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=4.892e+00, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:40:05,366:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=2.446e+00, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-06-30 09:40:05,367:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 59 iterations, alpha=2.372e+00, previous alpha=2.372e+00, with an active set of 36 regressors.
  warnings.warn(

2023-06-30 09:40:05,399:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=4.497e+01, previous alpha=4.387e+01, with an active set of 23 regressors.
  warnings.warn(

2023-06-30 09:40:07,613:INFO:Calculating mean and std
2023-06-30 09:40:07,615:INFO:Creating metrics dataframe
2023-06-30 09:40:08,069:INFO:Uploading results into container
2023-06-30 09:40:08,070:INFO:Uploading model into container now
2023-06-30 09:40:08,071:INFO:_master_model_container: 6
2023-06-30 09:40:08,071:INFO:_display_container: 2
2023-06-30 09:40:08,071:INFO:LassoLars(random_state=6820)
2023-06-30 09:40:08,072:INFO:create_model() successfully completed......................................
2023-06-30 09:40:08,182:INFO:SubProcess create_model() end ==================================
2023-06-30 09:40:08,182:INFO:Creating metrics dataframe
2023-06-30 09:40:08,194:INFO:Initializing Orthogonal Matching Pursuit
2023-06-30 09:40:08,195:INFO:Total runtime is 0.759536369641622 minutes
2023-06-30 09:40:08,199:INFO:SubProcess create_model() called ==================================
2023-06-30 09:40:08,200:INFO:Initializing create_model()
2023-06-30 09:40:08,200:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000245D8A272E0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 09:40:08,200:INFO:Checking exceptions
2023-06-30 09:40:08,200:INFO:Importing libraries
2023-06-30 09:40:08,200:INFO:Copying training dataset
2023-06-30 09:40:08,209:INFO:Defining folds
2023-06-30 09:40:08,209:INFO:Declaring metric variables
2023-06-30 09:40:08,214:INFO:Importing untrained model
2023-06-30 09:40:08,220:INFO:Orthogonal Matching Pursuit Imported successfully
2023-06-30 09:40:08,231:INFO:Starting cross validation
2023-06-30 09:40:08,234:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 09:40:13,181:INFO:Calculating mean and std
2023-06-30 09:40:13,183:INFO:Creating metrics dataframe
2023-06-30 09:40:13,639:INFO:Uploading results into container
2023-06-30 09:40:13,642:INFO:Uploading model into container now
2023-06-30 09:40:13,643:INFO:_master_model_container: 7
2023-06-30 09:40:13,643:INFO:_display_container: 2
2023-06-30 09:40:13,643:INFO:OrthogonalMatchingPursuit()
2023-06-30 09:40:13,643:INFO:create_model() successfully completed......................................
2023-06-30 09:40:13,753:INFO:SubProcess create_model() end ==================================
2023-06-30 09:40:13,753:INFO:Creating metrics dataframe
2023-06-30 09:40:13,770:INFO:Initializing Bayesian Ridge
2023-06-30 09:40:13,770:INFO:Total runtime is 0.8524546742439271 minutes
2023-06-30 09:40:13,775:INFO:SubProcess create_model() called ==================================
2023-06-30 09:40:13,776:INFO:Initializing create_model()
2023-06-30 09:40:13,776:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000245D8A272E0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 09:40:13,777:INFO:Checking exceptions
2023-06-30 09:40:13,777:INFO:Importing libraries
2023-06-30 09:40:13,777:INFO:Copying training dataset
2023-06-30 09:40:13,784:INFO:Defining folds
2023-06-30 09:40:13,784:INFO:Declaring metric variables
2023-06-30 09:40:13,791:INFO:Importing untrained model
2023-06-30 09:40:13,797:INFO:Bayesian Ridge Imported successfully
2023-06-30 09:40:13,805:INFO:Starting cross validation
2023-06-30 09:40:13,811:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 09:40:19,028:INFO:Calculating mean and std
2023-06-30 09:40:19,030:INFO:Creating metrics dataframe
2023-06-30 09:40:19,493:INFO:Uploading results into container
2023-06-30 09:40:19,494:INFO:Uploading model into container now
2023-06-30 09:40:19,494:INFO:_master_model_container: 8
2023-06-30 09:40:19,495:INFO:_display_container: 2
2023-06-30 09:40:19,495:INFO:BayesianRidge()
2023-06-30 09:40:19,495:INFO:create_model() successfully completed......................................
2023-06-30 09:40:19,603:INFO:SubProcess create_model() end ==================================
2023-06-30 09:40:19,603:INFO:Creating metrics dataframe
2023-06-30 09:40:19,617:INFO:Initializing Passive Aggressive Regressor
2023-06-30 09:40:19,617:INFO:Total runtime is 0.949911916255951 minutes
2023-06-30 09:40:19,621:INFO:SubProcess create_model() called ==================================
2023-06-30 09:40:19,621:INFO:Initializing create_model()
2023-06-30 09:40:19,621:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000245D8A272E0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 09:40:19,621:INFO:Checking exceptions
2023-06-30 09:40:19,621:INFO:Importing libraries
2023-06-30 09:40:19,624:INFO:Copying training dataset
2023-06-30 09:40:19,630:INFO:Defining folds
2023-06-30 09:40:19,631:INFO:Declaring metric variables
2023-06-30 09:40:19,636:INFO:Importing untrained model
2023-06-30 09:40:19,642:INFO:Passive Aggressive Regressor Imported successfully
2023-06-30 09:40:19,653:INFO:Starting cross validation
2023-06-30 09:40:19,656:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 09:40:25,582:INFO:Calculating mean and std
2023-06-30 09:40:25,584:INFO:Creating metrics dataframe
2023-06-30 09:40:26,054:INFO:Uploading results into container
2023-06-30 09:40:26,055:INFO:Uploading model into container now
2023-06-30 09:40:26,055:INFO:_master_model_container: 9
2023-06-30 09:40:26,057:INFO:_display_container: 2
2023-06-30 09:40:26,058:INFO:PassiveAggressiveRegressor(random_state=6820)
2023-06-30 09:40:26,058:INFO:create_model() successfully completed......................................
2023-06-30 09:40:26,169:INFO:SubProcess create_model() end ==================================
2023-06-30 09:40:26,169:INFO:Creating metrics dataframe
2023-06-30 09:40:26,187:INFO:Initializing Huber Regressor
2023-06-30 09:40:26,187:INFO:Total runtime is 1.0594092011451721 minutes
2023-06-30 09:40:26,192:INFO:SubProcess create_model() called ==================================
2023-06-30 09:40:26,192:INFO:Initializing create_model()
2023-06-30 09:40:26,193:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000245D8A272E0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 09:40:26,193:INFO:Checking exceptions
2023-06-30 09:40:26,193:INFO:Importing libraries
2023-06-30 09:40:26,193:INFO:Copying training dataset
2023-06-30 09:40:26,200:INFO:Defining folds
2023-06-30 09:40:26,200:INFO:Declaring metric variables
2023-06-30 09:40:26,209:INFO:Importing untrained model
2023-06-30 09:40:26,214:INFO:Huber Regressor Imported successfully
2023-06-30 09:40:26,225:INFO:Starting cross validation
2023-06-30 09:40:26,228:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 09:40:26,680:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-30 09:40:26,680:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-30 09:40:26,680:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-30 09:40:28,183:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-30 09:40:28,184:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-30 09:40:28,219:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-30 09:40:28,394:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-30 09:40:29,480:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-30 09:40:29,617:WARNING:c:\Python310\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-06-30 09:40:31,840:INFO:Calculating mean and std
2023-06-30 09:40:31,843:INFO:Creating metrics dataframe
2023-06-30 09:40:32,327:INFO:Uploading results into container
2023-06-30 09:40:32,328:INFO:Uploading model into container now
2023-06-30 09:40:32,329:INFO:_master_model_container: 10
2023-06-30 09:40:32,329:INFO:_display_container: 2
2023-06-30 09:40:32,330:INFO:HuberRegressor()
2023-06-30 09:40:32,330:INFO:create_model() successfully completed......................................
2023-06-30 09:40:32,442:INFO:SubProcess create_model() end ==================================
2023-06-30 09:40:32,442:INFO:Creating metrics dataframe
2023-06-30 09:40:32,455:INFO:Initializing K Neighbors Regressor
2023-06-30 09:40:32,455:INFO:Total runtime is 1.1638713121414184 minutes
2023-06-30 09:40:32,461:INFO:SubProcess create_model() called ==================================
2023-06-30 09:40:32,461:INFO:Initializing create_model()
2023-06-30 09:40:32,461:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000245D8A272E0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 09:40:32,462:INFO:Checking exceptions
2023-06-30 09:40:32,462:INFO:Importing libraries
2023-06-30 09:40:32,462:INFO:Copying training dataset
2023-06-30 09:40:32,468:INFO:Defining folds
2023-06-30 09:40:32,468:INFO:Declaring metric variables
2023-06-30 09:40:32,474:INFO:Importing untrained model
2023-06-30 09:40:32,480:INFO:K Neighbors Regressor Imported successfully
2023-06-30 09:40:32,491:INFO:Starting cross validation
2023-06-30 09:40:32,494:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 09:40:37,800:INFO:Calculating mean and std
2023-06-30 09:40:37,801:INFO:Creating metrics dataframe
2023-06-30 09:40:38,270:INFO:Uploading results into container
2023-06-30 09:40:38,273:INFO:Uploading model into container now
2023-06-30 09:40:38,275:INFO:_master_model_container: 11
2023-06-30 09:40:38,275:INFO:_display_container: 2
2023-06-30 09:40:38,275:INFO:KNeighborsRegressor(n_jobs=-1)
2023-06-30 09:40:38,276:INFO:create_model() successfully completed......................................
2023-06-30 09:40:38,382:INFO:SubProcess create_model() end ==================================
2023-06-30 09:40:38,382:INFO:Creating metrics dataframe
2023-06-30 09:40:38,402:INFO:Initializing Decision Tree Regressor
2023-06-30 09:40:38,403:INFO:Total runtime is 1.263017737865448 minutes
2023-06-30 09:40:38,410:INFO:SubProcess create_model() called ==================================
2023-06-30 09:40:38,411:INFO:Initializing create_model()
2023-06-30 09:40:38,411:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000245D8A272E0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 09:40:38,411:INFO:Checking exceptions
2023-06-30 09:40:38,411:INFO:Importing libraries
2023-06-30 09:40:38,412:INFO:Copying training dataset
2023-06-30 09:40:38,419:INFO:Defining folds
2023-06-30 09:40:38,419:INFO:Declaring metric variables
2023-06-30 09:40:38,426:INFO:Importing untrained model
2023-06-30 09:40:38,432:INFO:Decision Tree Regressor Imported successfully
2023-06-30 09:40:38,442:INFO:Starting cross validation
2023-06-30 09:40:38,446:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 09:40:43,513:INFO:Calculating mean and std
2023-06-30 09:40:43,515:INFO:Creating metrics dataframe
2023-06-30 09:40:43,989:INFO:Uploading results into container
2023-06-30 09:40:43,990:INFO:Uploading model into container now
2023-06-30 09:40:43,991:INFO:_master_model_container: 12
2023-06-30 09:40:43,991:INFO:_display_container: 2
2023-06-30 09:40:43,991:INFO:DecisionTreeRegressor(random_state=6820)
2023-06-30 09:40:43,991:INFO:create_model() successfully completed......................................
2023-06-30 09:40:44,097:INFO:SubProcess create_model() end ==================================
2023-06-30 09:40:44,097:INFO:Creating metrics dataframe
2023-06-30 09:40:44,117:INFO:Initializing Random Forest Regressor
2023-06-30 09:40:44,117:INFO:Total runtime is 1.3582526723543802 minutes
2023-06-30 09:40:44,124:INFO:SubProcess create_model() called ==================================
2023-06-30 09:40:44,124:INFO:Initializing create_model()
2023-06-30 09:40:44,125:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000245D8A272E0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 09:40:44,125:INFO:Checking exceptions
2023-06-30 09:40:44,125:INFO:Importing libraries
2023-06-30 09:40:44,125:INFO:Copying training dataset
2023-06-30 09:40:44,131:INFO:Defining folds
2023-06-30 09:40:44,131:INFO:Declaring metric variables
2023-06-30 09:40:44,137:INFO:Importing untrained model
2023-06-30 09:40:44,144:INFO:Random Forest Regressor Imported successfully
2023-06-30 09:40:44,153:INFO:Starting cross validation
2023-06-30 09:40:44,157:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 09:40:49,729:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:40:49,812:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:40:54,342:INFO:Calculating mean and std
2023-06-30 09:40:54,344:INFO:Creating metrics dataframe
2023-06-30 09:40:54,833:INFO:Uploading results into container
2023-06-30 09:40:54,834:INFO:Uploading model into container now
2023-06-30 09:40:54,834:INFO:_master_model_container: 13
2023-06-30 09:40:54,834:INFO:_display_container: 2
2023-06-30 09:40:54,835:INFO:RandomForestRegressor(n_jobs=-1, random_state=6820)
2023-06-30 09:40:54,835:INFO:create_model() successfully completed......................................
2023-06-30 09:40:54,945:INFO:SubProcess create_model() end ==================================
2023-06-30 09:40:54,946:INFO:Creating metrics dataframe
2023-06-30 09:40:54,966:INFO:Initializing Extra Trees Regressor
2023-06-30 09:40:54,966:INFO:Total runtime is 1.5390594561894733 minutes
2023-06-30 09:40:54,972:INFO:SubProcess create_model() called ==================================
2023-06-30 09:40:54,973:INFO:Initializing create_model()
2023-06-30 09:40:54,973:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000245D8A272E0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 09:40:54,973:INFO:Checking exceptions
2023-06-30 09:40:54,973:INFO:Importing libraries
2023-06-30 09:40:54,973:INFO:Copying training dataset
2023-06-30 09:40:54,980:INFO:Defining folds
2023-06-30 09:40:54,980:INFO:Declaring metric variables
2023-06-30 09:40:54,985:INFO:Importing untrained model
2023-06-30 09:40:54,991:INFO:Extra Trees Regressor Imported successfully
2023-06-30 09:40:55,001:INFO:Starting cross validation
2023-06-30 09:40:55,004:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 09:40:59,435:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:41:03,790:INFO:Calculating mean and std
2023-06-30 09:41:03,792:INFO:Creating metrics dataframe
2023-06-30 09:41:04,281:INFO:Uploading results into container
2023-06-30 09:41:04,282:INFO:Uploading model into container now
2023-06-30 09:41:04,283:INFO:_master_model_container: 14
2023-06-30 09:41:04,283:INFO:_display_container: 2
2023-06-30 09:41:04,283:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=6820)
2023-06-30 09:41:04,283:INFO:create_model() successfully completed......................................
2023-06-30 09:41:04,393:INFO:SubProcess create_model() end ==================================
2023-06-30 09:41:04,393:INFO:Creating metrics dataframe
2023-06-30 09:41:04,408:INFO:Initializing AdaBoost Regressor
2023-06-30 09:41:04,408:INFO:Total runtime is 1.6964317361513772 minutes
2023-06-30 09:41:04,414:INFO:SubProcess create_model() called ==================================
2023-06-30 09:41:04,414:INFO:Initializing create_model()
2023-06-30 09:41:04,414:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000245D8A272E0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 09:41:04,414:INFO:Checking exceptions
2023-06-30 09:41:04,414:INFO:Importing libraries
2023-06-30 09:41:04,415:INFO:Copying training dataset
2023-06-30 09:41:04,422:INFO:Defining folds
2023-06-30 09:41:04,422:INFO:Declaring metric variables
2023-06-30 09:41:04,427:INFO:Importing untrained model
2023-06-30 09:41:04,433:INFO:AdaBoost Regressor Imported successfully
2023-06-30 09:41:04,442:INFO:Starting cross validation
2023-06-30 09:41:04,444:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 09:41:10,351:INFO:Calculating mean and std
2023-06-30 09:41:10,353:INFO:Creating metrics dataframe
2023-06-30 09:41:10,857:INFO:Uploading results into container
2023-06-30 09:41:10,858:INFO:Uploading model into container now
2023-06-30 09:41:10,858:INFO:_master_model_container: 15
2023-06-30 09:41:10,859:INFO:_display_container: 2
2023-06-30 09:41:10,859:INFO:AdaBoostRegressor(random_state=6820)
2023-06-30 09:41:10,859:INFO:create_model() successfully completed......................................
2023-06-30 09:41:10,965:INFO:SubProcess create_model() end ==================================
2023-06-30 09:41:10,965:INFO:Creating metrics dataframe
2023-06-30 09:41:10,984:INFO:Initializing Gradient Boosting Regressor
2023-06-30 09:41:10,984:INFO:Total runtime is 1.806036372979482 minutes
2023-06-30 09:41:10,990:INFO:SubProcess create_model() called ==================================
2023-06-30 09:41:10,990:INFO:Initializing create_model()
2023-06-30 09:41:10,990:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000245D8A272E0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 09:41:10,991:INFO:Checking exceptions
2023-06-30 09:41:10,991:INFO:Importing libraries
2023-06-30 09:41:10,991:INFO:Copying training dataset
2023-06-30 09:41:10,997:INFO:Defining folds
2023-06-30 09:41:10,998:INFO:Declaring metric variables
2023-06-30 09:41:11,003:INFO:Importing untrained model
2023-06-30 09:41:11,010:INFO:Gradient Boosting Regressor Imported successfully
2023-06-30 09:41:11,018:INFO:Starting cross validation
2023-06-30 09:41:11,021:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 09:41:17,946:INFO:Calculating mean and std
2023-06-30 09:41:17,948:INFO:Creating metrics dataframe
2023-06-30 09:41:18,499:INFO:Uploading results into container
2023-06-30 09:41:18,500:INFO:Uploading model into container now
2023-06-30 09:41:18,501:INFO:_master_model_container: 16
2023-06-30 09:41:18,501:INFO:_display_container: 2
2023-06-30 09:41:18,502:INFO:GradientBoostingRegressor(random_state=6820)
2023-06-30 09:41:18,502:INFO:create_model() successfully completed......................................
2023-06-30 09:41:18,640:INFO:SubProcess create_model() end ==================================
2023-06-30 09:41:18,640:INFO:Creating metrics dataframe
2023-06-30 09:41:18,658:INFO:Initializing Light Gradient Boosting Machine
2023-06-30 09:41:18,659:INFO:Total runtime is 1.9339465061823526 minutes
2023-06-30 09:41:18,666:INFO:SubProcess create_model() called ==================================
2023-06-30 09:41:18,667:INFO:Initializing create_model()
2023-06-30 09:41:18,667:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000245D8A272E0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 09:41:18,667:INFO:Checking exceptions
2023-06-30 09:41:18,667:INFO:Importing libraries
2023-06-30 09:41:18,668:INFO:Copying training dataset
2023-06-30 09:41:18,692:INFO:Defining folds
2023-06-30 09:41:18,693:INFO:Declaring metric variables
2023-06-30 09:41:18,699:INFO:Importing untrained model
2023-06-30 09:41:18,707:INFO:Light Gradient Boosting Machine Imported successfully
2023-06-30 09:41:18,720:INFO:Starting cross validation
2023-06-30 09:41:18,724:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 09:41:25,127:INFO:Calculating mean and std
2023-06-30 09:41:25,129:INFO:Creating metrics dataframe
2023-06-30 09:41:25,653:INFO:Uploading results into container
2023-06-30 09:41:25,655:INFO:Uploading model into container now
2023-06-30 09:41:25,656:INFO:_master_model_container: 17
2023-06-30 09:41:25,657:INFO:_display_container: 2
2023-06-30 09:41:25,657:INFO:LGBMRegressor(random_state=6820)
2023-06-30 09:41:25,658:INFO:create_model() successfully completed......................................
2023-06-30 09:41:25,766:INFO:SubProcess create_model() end ==================================
2023-06-30 09:41:25,766:INFO:Creating metrics dataframe
2023-06-30 09:41:25,785:INFO:Initializing Dummy Regressor
2023-06-30 09:41:25,785:INFO:Total runtime is 2.0527117172876994 minutes
2023-06-30 09:41:25,791:INFO:SubProcess create_model() called ==================================
2023-06-30 09:41:25,791:INFO:Initializing create_model()
2023-06-30 09:41:25,791:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000245D8A272E0>, model_only=True, return_train_score=False, kwargs={})
2023-06-30 09:41:25,791:INFO:Checking exceptions
2023-06-30 09:41:25,792:INFO:Importing libraries
2023-06-30 09:41:25,792:INFO:Copying training dataset
2023-06-30 09:41:25,799:INFO:Defining folds
2023-06-30 09:41:25,799:INFO:Declaring metric variables
2023-06-30 09:41:25,804:INFO:Importing untrained model
2023-06-30 09:41:25,812:INFO:Dummy Regressor Imported successfully
2023-06-30 09:41:25,824:INFO:Starting cross validation
2023-06-30 09:41:25,826:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 09:41:31,991:INFO:Calculating mean and std
2023-06-30 09:41:31,992:INFO:Creating metrics dataframe
2023-06-30 09:41:32,517:INFO:Uploading results into container
2023-06-30 09:41:32,518:INFO:Uploading model into container now
2023-06-30 09:41:32,519:INFO:_master_model_container: 18
2023-06-30 09:41:32,520:INFO:_display_container: 2
2023-06-30 09:41:32,520:INFO:DummyRegressor()
2023-06-30 09:41:32,520:INFO:create_model() successfully completed......................................
2023-06-30 09:41:32,626:INFO:SubProcess create_model() end ==================================
2023-06-30 09:41:32,626:INFO:Creating metrics dataframe
2023-06-30 09:41:32,661:INFO:Initializing create_model()
2023-06-30 09:41:32,661:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=GradientBoostingRegressor(random_state=6820), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-06-30 09:41:32,661:INFO:Checking exceptions
2023-06-30 09:41:32,664:INFO:Importing libraries
2023-06-30 09:41:32,664:INFO:Copying training dataset
2023-06-30 09:41:32,671:INFO:Defining folds
2023-06-30 09:41:32,672:INFO:Declaring metric variables
2023-06-30 09:41:32,672:INFO:Importing untrained model
2023-06-30 09:41:32,672:INFO:Declaring custom model
2023-06-30 09:41:32,673:INFO:Gradient Boosting Regressor Imported successfully
2023-06-30 09:41:32,675:INFO:Cross validation set to False
2023-06-30 09:41:32,675:INFO:Fitting Model
2023-06-30 09:41:33,760:INFO:GradientBoostingRegressor(random_state=6820)
2023-06-30 09:41:33,760:INFO:create_model() successfully completed......................................
2023-06-30 09:41:33,914:INFO:_master_model_container: 18
2023-06-30 09:41:33,914:INFO:_display_container: 2
2023-06-30 09:41:33,914:INFO:GradientBoostingRegressor(random_state=6820)
2023-06-30 09:41:33,915:INFO:compare_models() successfully completed......................................
2023-06-30 09:41:34,043:INFO:Initializing create_model()
2023-06-30 09:41:34,043:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=gbr, fold=10, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-06-30 09:41:34,043:INFO:Checking exceptions
2023-06-30 09:41:34,064:INFO:Importing libraries
2023-06-30 09:41:34,064:INFO:Copying training dataset
2023-06-30 09:41:34,074:INFO:Defining folds
2023-06-30 09:41:34,074:INFO:Declaring metric variables
2023-06-30 09:41:34,080:INFO:Importing untrained model
2023-06-30 09:41:34,088:INFO:Gradient Boosting Regressor Imported successfully
2023-06-30 09:41:34,100:INFO:Starting cross validation
2023-06-30 09:41:34,104:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 09:41:41,025:INFO:Calculating mean and std
2023-06-30 09:41:41,028:INFO:Creating metrics dataframe
2023-06-30 09:41:41,041:INFO:Finalizing model
2023-06-30 09:41:41,831:INFO:Uploading results into container
2023-06-30 09:41:41,832:INFO:Uploading model into container now
2023-06-30 09:41:41,850:INFO:_master_model_container: 19
2023-06-30 09:41:41,850:INFO:_display_container: 3
2023-06-30 09:41:41,852:INFO:GradientBoostingRegressor(random_state=6820)
2023-06-30 09:41:41,852:INFO:create_model() successfully completed......................................
2023-06-30 09:41:42,247:INFO:Initializing tune_model()
2023-06-30 09:41:42,247:INFO:tune_model(estimator=GradientBoostingRegressor(random_state=6820), fold=10, round=4, n_iter=30, custom_grid={'learning_rate': [0.01, 0.02, 0.05], 'max_depth': [1, 2, 3, 4, 5, 6, 7, 8], 'subsample': [0.4, 0.5, 0.6, 0.7, 0.8], 'n_estimators': [100, 200.3, 400, 500, 600]}, optimize=RMSE, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>)
2023-06-30 09:41:42,247:INFO:Checking exceptions
2023-06-30 09:41:42,272:INFO:Copying training dataset
2023-06-30 09:41:42,278:INFO:Checking base model
2023-06-30 09:41:42,279:INFO:Base model : Gradient Boosting Regressor
2023-06-30 09:41:42,285:INFO:Declaring metric variables
2023-06-30 09:41:42,292:INFO:Defining Hyperparameters
2023-06-30 09:41:42,399:INFO:custom_grid: {'actual_estimator__learning_rate': [0.01, 0.02, 0.05], 'actual_estimator__max_depth': [1, 2, 3, 4, 5, 6, 7, 8], 'actual_estimator__subsample': [0.4, 0.5, 0.6, 0.7, 0.8], 'actual_estimator__n_estimators': [100, 200.3, 400, 500, 600]}
2023-06-30 09:41:42,399:INFO:Tuning with n_jobs=-1
2023-06-30 09:41:42,399:INFO:Initializing RandomizedSearchCV
2023-06-30 09:41:48,244:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.78s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:41:48,337:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:41:48,419:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:41:48,526:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:41:49,404:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:41:49,508:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:41:49,599:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:41:49,826:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:41:55,656:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:41:55,838:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:41:56,032:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:41:56,236:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:41:56,818:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:41:57,091:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:41:57,298:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:41:57,465:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:42:02,892:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.77s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:42:03,146:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.92s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:42:03,743:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.82s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:42:03,978:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 1.03s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:42:05,260:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:42:05,372:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:42:07,927:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:42:07,970:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:42:09,241:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:42:09,989:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:42:10,095:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:42:14,743:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:42:15,207:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:42:15,406:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:42:31,913:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:42:31,996:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:42:32,857:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:42:33,171:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:42:33,835:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:42:34,133:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:42:35,047:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:42:37,370:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:42:38,237:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:42:40,044:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:42:41,096:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:42:47,954:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:42:51,639:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:42:51,920:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:42:56,085:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:42:57,352:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:42:57,405:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:42:58,800:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:43:00,477:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:43:01,068:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:43:01,194:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:43:01,649:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:43:02,298:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:43:02,337:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:43:02,538:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:43:03,770:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.75s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:43:05,307:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:43:05,957:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:43:06,071:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:43:06,492:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:43:07,249:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:43:07,317:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:43:07,334:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:43:08,767:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.81s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:43:23,206:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:43:23,268:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:43:24,017:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:43:24,848:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:43:25,921:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:43:30,850:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:43:31,334:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:43:34,586:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:43:35,467:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:43:35,671:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:43:36,682:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:43:42,246:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:43:43,265:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:43:43,455:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:43:44,123:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:43:44,435:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:43:45,255:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:43:45,364:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:43:46,488:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:43:50,704:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:43:51,052:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:43:51,850:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:43:51,896:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:43:53,066:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:43:55,616:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:43:56,674:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:43:57,428:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:44:02,452:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:44:02,712:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:44:07,461:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:44:07,607:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:44:13,380:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:44:13,381:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:44:14,039:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:44:14,386:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:44:14,505:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:44:14,559:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:44:15,265:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:44:15,661:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.61s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:44:21,455:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:44:21,678:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:44:22,152:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:44:22,586:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:44:22,631:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.60s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:44:22,932:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:44:23,430:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:44:23,892:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:44:30,649:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:44:31,209:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.70s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:44:32,054:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:44:32,558:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:45:05,334:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:45:05,598:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:45:06,384:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:45:07,493:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:45:08,424:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:45:10,628:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:45:11,894:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.56s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:45:12,607:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:45:12,891:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:45:15,998:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:45:52,055:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:45:52,055:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.58s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:45:53,166:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.57s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:45:53,208:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:45:54,328:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:45:55,416:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:45:56,571:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:45:57,729:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:05,074:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:06,098:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:07,514:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:08,491:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:09,011:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.59s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:46:10,031:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:19,108:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:46:19,947:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.50s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:46:20,186:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:20,857:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:46:21,348:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:22,264:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.68s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:46:22,294:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:23,584:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:29,592:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.54s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:30,220:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:46:30,483:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:31,521:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:46:31,610:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:32,894:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:38,966:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.64s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:46:39,554:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:46:40,359:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:40,811:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.62s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:43,251:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:46:44,484:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:46:44,739:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:45,673:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.65s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:46:45,938:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:46,866:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.72s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:46:47,234:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:48,237:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:49,464:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:46:50,755:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:46:50,921:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.69s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:51,859:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:46:52,259:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.76s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:52,728:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.66s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:46:53,307:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.67s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:54,198:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.80s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:55,551:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.73s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:46:57,026:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.74s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:46:57,089:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:260: UserWarning: Persisting input arguments took 0.63s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  fitted_estimator = self._memory_fit(

2023-06-30 09:46:58,465:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.71s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:47:17,200:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py:378: FitFailedWarning: 
50 fits failed out of a total of 300.
The score on these train-test partitions for these parameters will be set to nan.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
50 fits failed with the following error:
Traceback (most recent call last):
  File "c:\Python310\lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "c:\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 260, in fit
    fitted_estimator = self._memory_fit(
  File "c:\Python310\lib\site-packages\joblib\memory.py", line 594, in __call__
    return self._cached_call(args, kwargs)[0]
  File "c:\Python310\lib\site-packages\pycaret\internal\memory.py", line 398, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "c:\Python310\lib\site-packages\pycaret\internal\memory.py", line 309, in call
    output = self.func(*args, **kwargs)
  File "c:\Python310\lib\site-packages\pycaret\internal\pipeline.py", line 66, in _fit_one
    transformer.fit(*args, **fit_params)
  File "c:\Python310\lib\site-packages\sklearn\ensemble\_gb.py", line 420, in fit
    self._validate_params()
  File "c:\Python310\lib\site-packages\sklearn\base.py", line 600, in _validate_params
    validate_parameter_constraints(
  File "c:\Python310\lib\site-packages\sklearn\utils\_param_validation.py", line 97, in validate_parameter_constraints
    raise InvalidParameterError(
sklearn.utils._param_validation.InvalidParameterError: The 'n_estimators' parameter of GradientBoostingRegressor must be an int in the range [1, inf). Got 200.3 instead.

  warnings.warn(some_fits_failed_message, FitFailedWarning)

2023-06-30 09:47:17,335:WARNING:c:\Python310\lib\site-packages\sklearn\model_selection\_search.py:952: UserWarning: One or more of the test scores are non-finite: [-5035.29784077 -4601.90105553 -4658.80967509 -6130.18407355
 -4706.81690876 -4763.35878613 -4500.09984355            nan
            nan -4803.17193712            nan -4849.86451314
 -4465.12540223 -4914.63302252 -4512.27866551 -6117.44861162
 -5924.63880473 -6116.47281089 -4537.36039221 -4541.21116329
 -4812.76775043 -4923.63289187            nan -4485.01798082
 -4475.68048387 -4970.5663086             nan -4520.34587627
 -4746.62661014 -4740.97527671]
  warnings.warn(

2023-06-30 09:47:18,129:INFO:best_params: {'actual_estimator__subsample': 0.8, 'actual_estimator__n_estimators': 400, 'actual_estimator__max_depth': 3, 'actual_estimator__learning_rate': 0.01}
2023-06-30 09:47:18,130:INFO:Hyperparameter search completed
2023-06-30 09:47:18,131:INFO:SubProcess create_model() called ==================================
2023-06-30 09:47:18,133:INFO:Initializing create_model()
2023-06-30 09:47:18,133:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=GradientBoostingRegressor(random_state=6820), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000245D3FEA4D0>, model_only=True, return_train_score=False, kwargs={'subsample': 0.8, 'n_estimators': 400, 'max_depth': 3, 'learning_rate': 0.01})
2023-06-30 09:47:18,133:INFO:Checking exceptions
2023-06-30 09:47:18,133:INFO:Importing libraries
2023-06-30 09:47:18,133:INFO:Copying training dataset
2023-06-30 09:47:18,139:INFO:Defining folds
2023-06-30 09:47:18,140:INFO:Declaring metric variables
2023-06-30 09:47:18,143:INFO:Importing untrained model
2023-06-30 09:47:18,143:INFO:Declaring custom model
2023-06-30 09:47:18,150:INFO:Gradient Boosting Regressor Imported successfully
2023-06-30 09:47:18,158:INFO:Starting cross validation
2023-06-30 09:47:18,161:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 09:47:22,187:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.53s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:47:22,189:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.51s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:47:22,207:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.55s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:47:22,308:WARNING:c:\Python310\lib\site-packages\pycaret\internal\pipeline.py:310: UserWarning: Persisting input arguments took 0.52s to run.
If this happens often in your code, it can cause performance problems 
(results will be correct in all cases). 
The reason for this is probably some large input arguments for a wrapped
 function (e.g. large strings).
THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an
 example so that they can fix the problem.
  X, _ = self._memory_full_transform(self, X, None, with_final=False)

2023-06-30 09:47:27,872:INFO:Calculating mean and std
2023-06-30 09:47:27,874:INFO:Creating metrics dataframe
2023-06-30 09:47:27,884:INFO:Finalizing model
2023-06-30 09:47:30,552:INFO:Uploading results into container
2023-06-30 09:47:30,553:INFO:Uploading model into container now
2023-06-30 09:47:30,554:INFO:_master_model_container: 20
2023-06-30 09:47:30,554:INFO:_display_container: 4
2023-06-30 09:47:30,555:INFO:GradientBoostingRegressor(learning_rate=0.01, n_estimators=400,
                          random_state=6820, subsample=0.8)
2023-06-30 09:47:30,555:INFO:create_model() successfully completed......................................
2023-06-30 09:47:30,665:INFO:SubProcess create_model() end ==================================
2023-06-30 09:47:30,666:INFO:choose_better activated
2023-06-30 09:47:30,669:INFO:SubProcess create_model() called ==================================
2023-06-30 09:47:30,670:INFO:Initializing create_model()
2023-06-30 09:47:30,670:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=GradientBoostingRegressor(random_state=6820), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-06-30 09:47:30,670:INFO:Checking exceptions
2023-06-30 09:47:30,674:INFO:Importing libraries
2023-06-30 09:47:30,674:INFO:Copying training dataset
2023-06-30 09:47:30,678:INFO:Defining folds
2023-06-30 09:47:30,678:INFO:Declaring metric variables
2023-06-30 09:47:30,679:INFO:Importing untrained model
2023-06-30 09:47:30,679:INFO:Declaring custom model
2023-06-30 09:47:30,680:INFO:Gradient Boosting Regressor Imported successfully
2023-06-30 09:47:30,681:INFO:Starting cross validation
2023-06-30 09:47:30,683:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-06-30 09:47:38,746:INFO:Calculating mean and std
2023-06-30 09:47:38,761:INFO:Creating metrics dataframe
2023-06-30 09:47:38,765:INFO:Finalizing model
2023-06-30 09:47:39,752:INFO:Uploading results into container
2023-06-30 09:47:39,753:INFO:Uploading model into container now
2023-06-30 09:47:39,754:INFO:_master_model_container: 21
2023-06-30 09:47:39,754:INFO:_display_container: 5
2023-06-30 09:47:39,754:INFO:GradientBoostingRegressor(random_state=6820)
2023-06-30 09:47:39,754:INFO:create_model() successfully completed......................................
2023-06-30 09:47:39,859:INFO:SubProcess create_model() end ==================================
2023-06-30 09:47:39,860:INFO:GradientBoostingRegressor(random_state=6820) result for RMSE is 4663.0356
2023-06-30 09:47:39,861:INFO:GradientBoostingRegressor(learning_rate=0.01, n_estimators=400,
                          random_state=6820, subsample=0.8) result for RMSE is 4465.1254
2023-06-30 09:47:39,862:INFO:GradientBoostingRegressor(learning_rate=0.01, n_estimators=400,
                          random_state=6820, subsample=0.8) is best model
2023-06-30 09:47:39,862:INFO:choose_better completed
2023-06-30 09:47:39,877:INFO:_master_model_container: 21
2023-06-30 09:47:39,877:INFO:_display_container: 4
2023-06-30 09:47:39,878:INFO:GradientBoostingRegressor(learning_rate=0.01, n_estimators=400,
                          random_state=6820, subsample=0.8)
2023-06-30 09:47:39,878:INFO:tune_model() successfully completed......................................
2023-06-30 09:47:40,788:INFO:Initializing predict_model()
2023-06-30 09:47:40,789:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=GradientBoostingRegressor(random_state=6820), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000245D43E2C20>)
2023-06-30 09:47:40,789:INFO:Checking exceptions
2023-06-30 09:47:40,789:INFO:Preloading libraries
2023-06-30 09:47:41,029:INFO:Initializing evaluate_model()
2023-06-30 09:47:41,029:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=GradientBoostingRegressor(random_state=6820), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-06-30 09:47:41,061:INFO:Initializing plot_model()
2023-06-30 09:47:41,061:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=6820), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, system=True)
2023-06-30 09:47:41,062:INFO:Checking exceptions
2023-06-30 09:47:41,067:INFO:Preloading libraries
2023-06-30 09:47:41,078:INFO:Copying training dataset
2023-06-30 09:47:41,078:INFO:Plot type: pipeline
2023-06-30 09:47:42,157:INFO:Visual Rendered Successfully
2023-06-30 09:47:42,288:INFO:plot_model() successfully completed......................................
2023-06-30 09:47:42,353:INFO:Initializing finalize_model()
2023-06-30 09:47:42,353:INFO:finalize_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=GradientBoostingRegressor(random_state=6820), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2023-06-30 09:47:42,354:INFO:Finalizing GradientBoostingRegressor(random_state=6820)
2023-06-30 09:47:42,359:INFO:Initializing create_model()
2023-06-30 09:47:42,362:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, estimator=GradientBoostingRegressor(random_state=6820), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, kwargs={})
2023-06-30 09:47:42,363:INFO:Checking exceptions
2023-06-30 09:47:42,365:INFO:Importing libraries
2023-06-30 09:47:42,366:INFO:Copying training dataset
2023-06-30 09:47:42,366:INFO:Defining folds
2023-06-30 09:47:42,366:INFO:Declaring metric variables
2023-06-30 09:47:42,367:INFO:Importing untrained model
2023-06-30 09:47:42,367:INFO:Declaring custom model
2023-06-30 09:47:42,368:INFO:Gradient Boosting Regressor Imported successfully
2023-06-30 09:47:42,370:INFO:Cross validation set to False
2023-06-30 09:47:42,370:INFO:Fitting Model
2023-06-30 09:47:43,253:INFO:Pipeline(memory=FastMemory(location=C:\Users\acer\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWr...
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('bin_numeric_features',
                 TransformerWrapper(include=['age', 'bmi'],
                                    transformer=KBinsDiscretizer(encode='ordinal',
                                                                 strategy='kmeans'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('actual_estimator',
                 GradientBoostingRegressor(random_state=6820))])
2023-06-30 09:47:43,253:INFO:create_model() successfully completed......................................
2023-06-30 09:47:43,364:INFO:_master_model_container: 21
2023-06-30 09:47:43,365:INFO:_display_container: 5
2023-06-30 09:47:43,419:INFO:Pipeline(memory=FastMemory(location=C:\Users\acer\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWr...
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('bin_numeric_features',
                 TransformerWrapper(include=['age', 'bmi'],
                                    transformer=KBinsDiscretizer(encode='ordinal',
                                                                 strategy='kmeans'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('actual_estimator',
                 GradientBoostingRegressor(random_state=6820))])
2023-06-30 09:47:43,419:INFO:finalize_model() successfully completed......................................
2023-06-30 09:47:43,692:INFO:Initializing save_model()
2023-06-30 09:47:43,692:INFO:save_model(model=Pipeline(memory=FastMemory(location=C:\Users\acer\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWr...
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('bin_numeric_features',
                 TransformerWrapper(include=['age', 'bmi'],
                                    transformer=KBinsDiscretizer(encode='ordinal',
                                                                 strategy='kmeans'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('actual_estimator',
                 GradientBoostingRegressor(random_state=6820))]), model_name=Premimum-Prediction_model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\acer\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWr...
                                    transformer=OneHotEncoder(cols=['region'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('bin_numeric_features',
                 TransformerWrapper(include=['age', 'bmi'],
                                    transformer=KBinsDiscretizer(encode='ordinal',
                                                                 strategy='kmeans'))),
                ('normalize',
                 TransformerWrapper(transformer=StandardScaler()))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2023-06-30 09:47:43,692:INFO:Adding model into prep_pipe
2023-06-30 09:47:43,692:WARNING:Only Model saved as it was a pipeline.
2023-06-30 09:47:43,713:INFO:Premimum-Prediction_model.pkl saved in current working directory
2023-06-30 09:47:43,771:INFO:Pipeline(memory=FastMemory(location=C:\Users\acer\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWr...
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('bin_numeric_features',
                 TransformerWrapper(include=['age', 'bmi'],
                                    transformer=KBinsDiscretizer(encode='ordinal',
                                                                 strategy='kmeans'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('actual_estimator',
                 GradientBoostingRegressor(random_state=6820))])
2023-06-30 09:47:43,772:INFO:save_model() successfully completed......................................
2023-06-30 09:48:07,974:INFO:Initializing plot_model()
2023-06-30 09:48:07,974:INFO:plot_model(plot=vc, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=6820), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, system=True)
2023-06-30 09:48:07,974:INFO:Checking exceptions
2023-06-30 09:48:07,980:INFO:Preloading libraries
2023-06-30 09:48:07,989:INFO:Copying training dataset
2023-06-30 09:48:07,989:INFO:Plot type: vc
2023-06-30 09:48:07,990:INFO:Determining param_name
2023-06-30 09:48:07,990:INFO:param_name: alpha
2023-06-30 09:48:08,153:INFO:Fitting Model
2023-06-30 09:48:19,225:INFO:Visual Rendered Successfully
2023-06-30 09:48:19,453:INFO:plot_model() successfully completed......................................
2023-06-30 09:48:19,533:INFO:Initializing plot_model()
2023-06-30 09:48:19,534:INFO:plot_model(plot=residuals_interactive, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=6820), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, system=True)
2023-06-30 09:48:19,534:INFO:Checking exceptions
2023-06-30 09:48:19,538:INFO:Preloading libraries
2023-06-30 09:48:19,549:INFO:Copying training dataset
2023-06-30 09:48:19,549:INFO:Plot type: residuals_interactive
2023-06-30 09:48:19,833:INFO:Calculated model residuals
2023-06-30 09:48:36,205:INFO:Calculated Tunkey-Anscombe Plot
2023-06-30 09:48:36,624:INFO:Calculated Normal QQ Plot
2023-06-30 09:48:36,999:INFO:Calculated Scale-Location Plot
2023-06-30 09:48:38,768:INFO:Initializing plot_model()
2023-06-30 09:48:38,768:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=6820), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, system=True)
2023-06-30 09:48:38,768:INFO:Checking exceptions
2023-06-30 09:48:38,774:INFO:Preloading libraries
2023-06-30 09:48:38,784:INFO:Copying training dataset
2023-06-30 09:48:38,785:INFO:Plot type: pipeline
2023-06-30 09:48:39,042:INFO:Visual Rendered Successfully
2023-06-30 09:48:39,209:INFO:plot_model() successfully completed......................................
2023-06-30 09:48:44,482:INFO:Initializing plot_model()
2023-06-30 09:48:44,482:INFO:plot_model(plot=tree, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=6820), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, system=True)
2023-06-30 09:48:44,482:INFO:Checking exceptions
2023-06-30 09:48:52,045:INFO:Initializing plot_model()
2023-06-30 09:48:52,045:INFO:plot_model(plot=error, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=6820), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, system=True)
2023-06-30 09:48:52,045:INFO:Checking exceptions
2023-06-30 09:48:52,049:INFO:Preloading libraries
2023-06-30 09:48:52,059:INFO:Copying training dataset
2023-06-30 09:48:52,059:INFO:Plot type: error
2023-06-30 09:48:52,280:INFO:Fitting Model
2023-06-30 09:48:52,280:WARNING:c:\Python310\lib\site-packages\sklearn\base.py:439: UserWarning:

X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names


2023-06-30 09:48:52,280:INFO:Scoring test/hold-out set
2023-06-30 09:48:53,452:INFO:Visual Rendered Successfully
2023-06-30 09:48:53,671:INFO:plot_model() successfully completed......................................
2023-06-30 09:49:03,524:INFO:Initializing plot_model()
2023-06-30 09:49:03,524:INFO:plot_model(plot=learning, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=6820), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x00000245B62F99F0>, system=True)
2023-06-30 09:49:03,524:INFO:Checking exceptions
2023-06-30 09:49:03,530:INFO:Preloading libraries
2023-06-30 09:49:03,539:INFO:Copying training dataset
2023-06-30 09:49:03,539:INFO:Plot type: learning
2023-06-30 09:49:03,607:INFO:Fitting Model
2023-06-30 09:49:12,629:INFO:Visual Rendered Successfully
2023-06-30 09:49:12,787:INFO:plot_model() successfully completed......................................
2023-06-30 09:52:29,896:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 09:52:29,896:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 09:52:29,896:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 09:52:29,896:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 09:52:40,903:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-06-30 09:52:55,643:INFO:Initializing load_model()
2023-06-30 09:52:55,643:INFO:load_model(model_name=Premimum-Prediction_model, platform=None, authentication=None, verbose=True)
2023-06-30 09:53:24,003:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 09:53:24,003:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 09:53:24,003:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 09:53:24,003:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-06-30 09:53:24,671:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2023-06-30 09:53:25,532:INFO:Initializing load_model()
2023-06-30 09:53:25,541:INFO:load_model(model_name=Premimum-Prediction_model, platform=None, authentication=None, verbose=True)
2023-06-30 09:53:33,665:INFO:Initializing load_model()
2023-06-30 09:53:33,665:INFO:load_model(model_name=Premimum-Prediction_model, platform=None, authentication=None, verbose=True)
2023-06-30 09:53:50,014:INFO:Initializing load_model()
2023-06-30 09:53:50,014:INFO:load_model(model_name=Premimum-Prediction_model, platform=None, authentication=None, verbose=True)
2023-06-30 09:53:50,288:INFO:Initializing predict_model()
2023-06-30 09:53:50,288:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000171DA4C3E50>, estimator=Pipeline(memory=FastMemory(location=C:\Users\acer\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWr...
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('bin_numeric_features',
                 TransformerWrapper(include=['age', 'bmi'],
                                    transformer=KBinsDiscretizer(encode='ordinal',
                                                                 strategy='kmeans'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('actual_estimator',
                 GradientBoostingRegressor(random_state=6820))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000171DA6B8CA0>)
2023-06-30 09:53:50,288:INFO:Checking exceptions
2023-06-30 09:53:50,288:INFO:Preloading libraries
2023-06-30 09:53:50,288:INFO:Set up data.
2023-06-30 09:53:50,303:INFO:Set up index.
2023-06-30 09:54:17,361:INFO:Initializing load_model()
2023-06-30 09:54:17,361:INFO:load_model(model_name=Premimum-Prediction_model, platform=None, authentication=None, verbose=True)
2023-06-30 09:54:17,555:INFO:Initializing load_model()
2023-06-30 09:54:17,555:INFO:load_model(model_name=Premimum-Prediction_model, platform=None, authentication=None, verbose=True)
2023-06-30 09:54:17,658:INFO:Initializing predict_model()
2023-06-30 09:54:17,658:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x00000171DA714310>, estimator=Pipeline(memory=FastMemory(location=C:\Users\acer\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['age', 'bmi', 'children'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['sex', 'smoker', 'region'],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('ordinal_encoding',
                 TransformerWr...
                ('polynomial_features',
                 TransformerWrapper(transformer=PolynomialFeatures(include_bias=False))),
                ('bin_numeric_features',
                 TransformerWrapper(include=['age', 'bmi'],
                                    transformer=KBinsDiscretizer(encode='ordinal',
                                                                 strategy='kmeans'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('actual_estimator',
                 GradientBoostingRegressor(random_state=6820))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000171DA9C9B40>)
2023-06-30 09:54:17,658:INFO:Checking exceptions
2023-06-30 09:54:17,658:INFO:Preloading libraries
2023-06-30 09:54:17,658:INFO:Set up data.
2023-06-30 09:54:17,673:INFO:Set up index.
2023-06-30 09:55:28,331:INFO:Initializing load_model()
2023-06-30 09:55:28,332:INFO:load_model(model_name=Premimum-Prediction_model, platform=None, authentication=None, verbose=True)
